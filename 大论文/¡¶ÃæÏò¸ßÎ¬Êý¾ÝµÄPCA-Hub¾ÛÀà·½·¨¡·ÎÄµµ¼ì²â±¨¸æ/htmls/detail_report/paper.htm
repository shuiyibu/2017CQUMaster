<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN""http://www.w3.org/TR/html4/loose.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta name="robots" content="nofollow"/>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<title>PaperFree 最权威中文论文抄袭检测系统</title>
<style type="text/css">
<!--
user_icon {
color: #FFFFFF;
}
html
{
overflow-x:hidden;
overflow-y:auto;
}
body,td,th {
font-family: "微软雅黑";
font-size: 12px;
}
h1,h2,h3,h4,h5,h6 {
font-family: "宋体";
}
p{
margin-bottom:10px;
}
demo_padding {
line-height: 30px;
}
.zhengwen {
padding-right: 15px;
padding-left: 5px;
padding-bottom:100px;
font-size: 13px;
line-height: 20px;
color: #666666;
}
.zhengwencenter {
padding-right: 15px;
padding-left: 0px;
margin-bottom:10px;
font-size: 13px;
line-height: 20px;
color: #666666;
text-align:center
}
.neikuang {
background-color: #EBEBEB;
border: 1px solid #999999;
padding-right: 10px;
padding-left: 10px;
margin-top:10px;
margin-left:25px;
width:300px;
}
.shubu{
height: 20px;
width: 20px;
margin-left:25px;
background-color: #FFFFFF;
border: 1px solid #999999;
text-align: center;
vertical-align: middle;
display: block;
color: #666666;
}
a.red:link {color:#FF0000}
a.red:visited {color:#FF0000}
a.red:hover {color:#000000}
a.red:active {color:#000000}

a.orange:link {color:#FF9900}
a.orange:visited {color:#FF9900}
a.orange:hover {color:#000000}
a.orange:active {color:#000000}

a.dark:link {color:#666666}
a.dark:visited {color:#666666}
a.dark:hover {color:#000000}
a.dark:active {color:#000000}

a.pagelink:hover {color:#000000}
a.pagelink:active {color:#000000}

.green{color:#008000}
.gray{color:#666666}
span.gray:hover {color:#000000}

.red{color:#FF0000}
span.red:hover {color:#000000}
.orange{color:#FF9900}
span.orange:hover {color:#000000}

a{TEXT-DECORATION:none}
a:hover{TEXT-DECORATION:underline;}
.conNum1{padding:0 5px;height:20px;border:1px solid #ccc;}
.paper .autotype3{color:#FF0000;}
.paper .autotype2{color:#FFA500;}
-->
</style>
<script type="text/javascript" src="jquery-1.8.2.min.js"></script>
</head>
<body>
<div class="zhengwen">
		    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>1</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>附    录</span><br><span class='green'>摘  要</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>2</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/0.htm' target='right'><span class='red'>机器学习(Machine Learning)是一门人工智能科学,</span></a><span class='green'></span><a href='../sentence_detail/1.htm' target='right'><span class='orange'>该领域的主要研究对象为人工智能,机器学习是通过机器自主学习的方式来处理人工智能中的问题,特别是如何在经验学习中改善具体算法的性能。</span></a><span class='green'>近几十年机器学习在概率论、计算复杂性理论、统计学、逼近论等领域均有发展,已形成一门多领域交叉学科。机器学习通过设计和分析让机器可以自主“学习”的算法以便从海量数据中自动分析出有价值的模式或规律,从而对未知数据进行预测。机器学习大致可以分为下面四种类别:</span><a href='../sentence_detail/2.htm' target='right'><span class='red'>监督学习(Supervised Learning)、无监督学习(Unsupervised Learning)、半监督学习(Semi-supervised Learning)以及增强学习(Reinforcement Learning)。</span></a><span class='green'></span><a href='../sentence_detail/3.htm' target='right'><span class='orange'>机器学习已广泛应用于诸多领域:数据挖掘、计算机视觉、搜索引擎、自然语言处理、语音和手写识别、生物特征识别、DNA序列测序、医学诊断、检测信用卡欺诈和证券市场分析等。</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>3</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/4.htm' target='right'><span class='orange'>聚类分析是机器学习中的一种无监督学习,</span></a><span class='green'></span><a href='../sentence_detail/5.htm' target='right'><span class='orange'>近些年来受到越来越多的关注。</span></a><span class='green'></span><a href='../sentence_detail/6.htm' target='right'><span class='red'>聚类分析(Cluster Analysis,</span></a><span class='green'></span><a href='../sentence_detail/7.htm' target='right'><span class='orange'>亦称为群集分析)是把相似的对象通过静态分类的方法分成不同的簇或子集,</span></a><span class='green'></span><a href='../sentence_detail/8.htm' target='right'><span class='orange'>使得在同一个簇中的对象都具有某些相似的属性。</span></a><span class='green'></span><a href='../sentence_detail/9.htm' target='right'><span class='orange'>传统的聚类分析算法大致可分为以下五种:划分聚类(Partitioning Clustering)、层次聚类(Hierarchical Clustering)、基于密度的聚类(Density-Based Clustering)、基于网格的聚类(Grid-Based Clustering)和基于模型的聚类(Model-Based Clustering)。</span></a><span class='green'>传统的聚类分析算法更倾向于在低维数据空间中进行聚类分析。</span><a href='../sentence_detail/10.htm' target='right'><span class='orange'>然而由于现实生活中数据的复杂性和多样性,</span></a><span class='green'>传统聚类算法在处理诸多现实问题和任务时,往往不能科学地进行聚类分析,尤其在高维数据和海量数据上更是如此。这是因为在高维数据空间中利用传统聚类算法时常常会出现下述两个问题:第一,</span><a href='../sentence_detail/11.htm' target='right'><span class='orange'>高维数据存在大量冗余、噪声的特征使得不可能在所有维中均存在簇;第二,高维空间中的数据分布十分稀疏,</span></a><span class='green'>其数据间的距离几乎相等。显然,</span><a href='../sentence_detail/12.htm' target='right'><span class='orange'>基于距离的传统聚类方法无法在高维空间中基于距离来构建簇。</span></a><span class='green'>这便是机器学习中令人头疼的维数灾难(Curse of Dimensionality)问题[1]。近年来,</span><a href='../sentence_detail/13.htm' target='right'><span class='orange'>“维数灾难”已成为机器学习的一个重要研究方向。</span></a><span class='green'></span><a href='../sentence_detail/14.htm' target='right'><span class='orange'>随着科技的发展使得数据获取变得愈加容易,而数据规模愈发庞大、复杂性越来越高,如海量Web文档、基因序列等,</span></a><span class='green'>其维数从数百到数千不等,甚至更高。</span><a href='../sentence_detail/15.htm' target='right'><span class='orange'>高维数据分析虽然十分具有挑战性,但是它在信息安全、金融、市场分析、反恐等领域均有很广泛的应用。</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>4</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>为了解决维数灾难的问题,本文引入了hubness这一全新的概念,并在原有的hub聚类算法上通过实验分析后,对hub算法进行了改进。Hubness这一概念是在2010年由Milosˇ Radovanovic ́等人提出的[2],hubness描述的是这样一种现象:在k近邻列表中,某些点容易频繁地出现在其它点的k近邻列表中。在数据集中样本点出现在其它点的k近邻列表中的次数称为k-occurrences,随着维度的增加,k-occurrences的分布会逐渐向右倾斜,这将会导致hubs的出现。Hubs通常是指具有非常高的k-occurrences的样本点,换言之,hubs易于频繁地出现在其它点的k近邻类列表中。通过探究这种现象的根源,发现这是高维数据空间数据统计分布的一种内在属性[2]。Milosˇ Radovanovic ́等人利用这种内在属性对基于距离度量的各种机器学习方法进行了深入的研究,</span><a href='../sentence_detail/16.htm' target='right'><span class='orange'>包括监督学习方法、半监督学习方法和无监督学习方法。</span></a><span class='green'>在无监督学习方面,hub聚类分析算法有以下四种:deterministic、probabilistic、hybrid和kernel。</span><a href='../sentence_detail/17.htm' target='right'><span class='orange'>这四种方法均为K-Means算法的扩展。</span></a><span class='green'></span><a href='../sentence_detail/18.htm' target='right'><span class='orange'>Hub聚类算法虽然可以在高维数据空间中进行聚类分析,</span></a><span class='green'>但是它却忽略了高维数据空间中的冗余和噪声数据,从而无法获得更优的簇结构以及更快的聚类收敛速度。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>5</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>本文针对hub聚类分析算法的上述问题,提出了一种PCA-Hub聚类分析算法用于解决高维数据空间中的冗余和噪声数据,以便获得更好的簇结构和更快的聚类收敛速度。PCA-Hub聚类算法是以k-occurrences的偏度与本征维数强烈正相关为理论基础,通过构建数据集的KNN邻域矩阵,以偏度的变化率作为降维依据选出理想的k个主成分,之后再对降维后的数据集进行聚类分析。实验结果表明,PCA-Hub聚类算法相比之前的聚类算法在轮廓系数上平均提高了15%;当数据集的维数或者k-occurrences的偏度较高时,PCA-Hub聚类算法对近邻数的选择不敏感;在实验环境和实验参数一致的情况下,PCA-Hub聚类算法的结果在很大程度上具有一致性。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>6</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>PCA-Hub聚类算法虽然可以很好地解决高维数据空间中的冗余和噪声数据,</span><a href='../sentence_detail/19.htm' target='right'><span class='orange'>然而随着数据集尺度和数据集维数的不断增加,</span></a><span class='green'>PCA-Hub聚类算法的耗时将会变得越来越严重甚至是不可接受。因此,本文提出了一种Quick PCA-Hub聚类分析算法分别从快速搜索k个主成分和快速搜索最近邻居两方面加快PCA-Hub算法的聚类分析速度。实验结果表明,Quick PCA-Hub聚类算法相比之前的聚类算法在轮廓系数上平均提高了8%;Quick PCA-Hub在高维数据空间中搜索理想的k个主成分时表现出了巨大的优势。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>7</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>重庆大学硕士学位论文</span><br><span class='green'>中文摘要</span><br><span class='green'>关键词:Hub聚类,高维数据,偏度,本征维度,主成分分析</span><br><span class='green'>II</span><br><span class='green'>I</span><br><span class='green'>ABSTRACT</span><br><span class='green'>W:</span><br><span class='green'>重庆大学硕士学位论文</span><br><span class='green'>英文摘要</span><br><span class='green'>Key words: wind blades, anti-icing, hydrophobic coatings, frozen speed, wind tune test, natural icing</span><br><span class='green'>IV</span><br><span class='green'>III</span><br><span class='green'>重庆大学硕士学位论文</span><br><span class='green'>目    录</span><br><span class='green'>绪 论</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>8</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/20.htm' target='right'><span class='orange'>本章主要介绍论文研究背景及其意义,</span></a><span class='green'>阐述论文研究方向及其主要内容,同时对论文的整体结构作简要说明。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>9</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>研究背景及意义</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>10</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>由于当今科学技术的发展越来越迅捷,并且云计算等新兴大数据处理技术也在计算机等诸多领域持续发展,因此人们对大型数据表现出前所未有的关注。信息网络的快速传播使得现实生活中数据几乎呈现出指数增长的趋势,随着网络数据的持续增加和网络数据结构的持续复杂,使得数据分析变得愈加困难。当今社会数据的过快产生使得我们身处在一个“被信息所淹没,但却渴望从中获取知识”的环境中[3]。对于这些大量、增长速度持续增加并且结构异常复杂的数据,</span><a href='../sentence_detail/21.htm' target='right'><span class='orange'>传统的数据处理方法已变得不再适用。</span></a><span class='green'>于是,</span><a href='../sentence_detail/22.htm' target='right'><span class='orange'>一种基于大数据的处理方法应运而生。</span></a><span class='green'></span><a href='../sentence_detail/23.htm' target='right'><span class='orange'>数据挖掘的主要目标是从大量数据中提取出有价值的模式和知识,</span></a><span class='green'>然后将其转变为人类可理解的结构,以便后续的工作使用[4]。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>11</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>在大型的数据集中,数据挖掘通过机器学习、人工智能、统计学等交叉方法从而发现有价值的模式和知识。数据挖掘的过程是对大型数据进行监督或半监督的分析,从而获得之前未知的有意义的潜在信息,例如数据的聚类(通过聚类分析)、数据的异常信息(通过利群点检测)和数据之间的联系(通过关联规则分析)。数据挖掘的对象的类型并无限制,可以使任意类型的数据,</span><a href='../sentence_detail/24.htm' target='right'><span class='orange'>不管是结构化的数据、半结构化的数据,</span></a><span class='green'>还是异构型的数据[5]。</span><a href='../sentence_detail/25.htm' target='right'><span class='orange'>数据挖掘的主要过程如图1.1所示。</span></a><span class='green'>数据挖掘的过程通常定义为以下三大阶段:第一、预处理阶段:在获取到目标数据集后,</span><a href='../sentence_detail/26.htm' target='right'><span class='orange'>有必要对多变量数据进行分析,</span></a><span class='green'>处理那些包含噪声和含有缺失数据的观测量;第二、数据挖掘阶段:数据挖掘过程通常涉及六种常见的任务,异常检测(异常/变化/偏差检测)、关联规则学习(依赖建模)、聚类、分析、回归以及汇总,这些均是利用数据挖掘技术从原有的数据集中发现未知的有价值信息;第三、结果验证阶段:通常,</span><a href='../sentence_detail/27.htm' target='right'><span class='orange'>数据挖掘是有目的地挖掘未知的有价值信息,然而这些信息是否符合预期一般可以通过结果验证来实现。</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>12</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/28.htm' target='right'><span class='red'>数据挖掘的方法包括监督式学习、无监督式学习、半监督学习以及增强学习。</span></a><span class='green'>监督学习是从已知的训练数据集中获得某种函数用于预测未知的数据集。</span><a href='../sentence_detail/29.htm' target='right'><span class='orange'>监督学习训练集中的目标是人为标注的。</span></a><span class='green'></span><a href='../sentence_detail/30.htm' target='right'><span class='red'>常见的监督式学习包括分类、估计、预测。</span></a><span class='green'></span><a href='../sentence_detail/31.htm' target='right'><span class='orange'>无监督学习与监督学习的不用之处在于训练集是没有人为标注的。</span></a><span class='green'></span><a href='../sentence_detail/32.htm' target='right'><span class='red'>常见的无监督式学习包括聚类、关联规则分析。</span></a><span class='green'></span><a href='../sentence_detail/33.htm' target='right'><span class='red'>半监督学习介于监督学习与无监督学习之间。</span></a><span class='green'>增强学习是基于环境而行动,从而获得最大化的预期利益。</span><a href='../sentence_detail/34.htm' target='right'><span class='orange'>聚类分析是一种常见的无监督式学习,</span></a><span class='green'></span><a href='../sentence_detail/35.htm' target='right'><span class='orange'>“物以类聚,人以群分”,无论是自然科学还是现实世界中均有各种各样的分类问题。</span></a><span class='green'></span><a href='../sentence_detail/36.htm' target='right'><span class='orange'>在数据挖掘中,聚类分析是研究分类问题的一种数据分析方法。</span></a><span class='green'></span><a href='../sentence_detail/37.htm' target='right'><span class='orange'>聚类分析是把大量复杂的数据通过聚类器将其分成若干不同的类别或更多的子集,</span></a><span class='green'>换言之,聚类分析的目的是尽可能地增大簇内部的相似性同时减小簇之间的相似性。</span><a href='../sentence_detail/38.htm' target='right'><span class='orange'>聚类分析在诸多领域均有应用,包括机器学习、数据挖掘、模式识别、图像分析以及生物信息等。</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>13</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>图 1.1 数据挖掘逻辑图</span><br><span class='green'>Figure 1.1 Data mining logic diagram</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>14</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>然而随着科学技术的发展,数据集的获取愈加便捷,</span><a href='../sentence_detail/39.htm' target='right'><span class='orange'>同时数据集的维数也不断增加。</span></a><span class='green'>虽然传统的聚类分析算法在低维数据空间可以获得良好的聚类效果,但是由于高维数据空间中数据的稀疏性以及距离集中等问题使得传统聚类算法难以进行科学的聚类分析。因此,</span><a href='../sentence_detail/40.htm' target='right'><span class='orange'>针对高维数据空间中的聚类分析变得很有意义。</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>15</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>国内外研究现状</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>16</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/41.htm' target='right'><span class='orange'>随着科学技术的发展,人们处理大型复杂数据的需求越来越强烈,</span></a><span class='green'>可以处理大型数据的数据挖掘在学术界也越来越受到关注。多年来,数据挖掘的相关理论不断完善和发展,而且其商业价值也逐步显现。</span><a href='../sentence_detail/42.htm' target='right'><span class='orange'>在数据挖掘中,聚类分析一直是其重要的组成部分,</span></a><span class='green'>自然也受到了研究者的高度关注。聚类分析是在1932年由两位人类学专家Driver和Kroeber首次提出的,1938年Zubin将其引入到了心理学领域。</span><a href='../sentence_detail/43.htm' target='right'><span class='orange'>就聚类分析本身而言,它并不是一个具体的算法,</span></a><span class='green'>而是处理某一类问题的通用规则。不同的聚类器可以定义不同的簇结构以及搜寻不同的簇的规则。主流的簇概念包括簇内对象之间的最小距离、数据空间的密集区域以及间隔或者特定的分布。</span><a href='../sentence_detail/44.htm' target='right'><span class='orange'>因此,聚类分析可以被表示为自动化地实现多目标的优化问题。</span></a><span class='green'>然而,</span><a href='../sentence_detail/45.htm' target='right'><span class='orange'>聚类分析本身并不是一个自动化的过程,</span></a><span class='green'>而是一个不断迭代的知识发现过程或是交互式多目标优化的过程。在这个迭代过程中需要不断修改数据预处理方式以及模型参数直到到达预期的结果。不同的数据集和不同的结果预期用途决定了聚类算法的选取和参数的设定(包括要使用的距离函数、密度阈值或者预期聚类的数量)。</span><a href='../sentence_detail/46.htm' target='right'><span class='orange'>当前主流的聚类分析算法可分为以下几类:</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>17</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>①层次聚类算法</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>18</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/47.htm' target='right'><span class='orange'>层次聚类算法也称作基于连通性的聚类算法,</span></a><span class='green'></span><a href='../sentence_detail/48.htm' target='right'><span class='orange'>其核心思想是若两个对象越接近,</span></a><span class='green'>那么它们的相关性就越强。这些算法基于对象间的距离将彼此连通从而形成不同的簇。在很大程度上,一个簇可以由该簇内的最大连通距离来表示。不同的距离会形成不同的簇,这可以通过树形结构来表示,这也是层次聚类名称的来源。通常而言,</span><a href='../sentence_detail/49.htm' target='right'><span class='orange'>层次聚类可分为两大类:自底向上(agglomerative)和自顶向下(divisive)。</span></a><span class='green'></span><a href='../sentence_detail/50.htm' target='right'><span class='orange'>自底向上是开始时所有数据点均各自为一个类别,然后每次迭代将距离最近的两个类合并,直到只有一个类为止。</span></a><span class='green'>自顶向下的思想与自底向上的思想正好完全相反。</span><a href='../sentence_detail/51.htm' target='right'><span class='orange'>虽然层次聚类的核心思想比较简单,但是计算复杂度却比较高。</span></a><span class='green'>因为上述的三种方法均需要计算所有点对之间的距离,</span><a href='../sentence_detail/52.htm' target='right'><span class='orange'>而且算法也表明每次迭代只能合并两个子类,这是非常耗时的。</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>19</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>②划分聚类算法</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>20</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>在基于中心的聚类中,由中心向量表示一个簇,该簇的中心元素不一定是数据集中的元素。当簇的个数为定值k时,K-Means聚类方法给出了关于最优化问题的形式定义:搜寻k个簇中心,然后将对象划分给与之最近的簇中心所在的簇。K-Means聚类算法的目标是最小化簇内平方和(WCSS within-cluster sum of squares)。</span><a href='../sentence_detail/53.htm' target='right'><span class='orange'>但是最优化问题本身是一个NP难问题,</span></a><span class='green'></span><a href='../sentence_detail/54.htm' target='right'><span class='orange'>因此常见的方法是找到其近似解。</span></a><span class='green'>最著名的近似方法是Lloyd's[6],也就是K-Means算法。然而,</span><a href='../sentence_detail/55.htm' target='right'><span class='orange'>K-Means算法只能找到局部最优值,</span></a><span class='green'>而且使用的是随机的初始值。</span><a href='../sentence_detail/56.htm' target='right'><span class='orange'>虽然K-Means算法算法有很多变形,</span></a><span class='green'>但是大多数基于K-Means算法的变形算法的最大缺点之一是需要预先指定簇的个数k。此外,由于这些算法均是基于簇中心的,所以更容易形成大小相似的簇,这通常会导致簇之间不正确的边界切割。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>21</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>③基于分布的聚类</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>22</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>与统计学最密切相关的聚类模型是基于分布的模型。对象的分布越相似,它们分配在同一个簇的可能性越大。</span><a href='../sentence_detail/57.htm' target='right'><span class='orange'>该方法易于处理类似于人工生成的数据集(从分布中随机取样)。</span></a><span class='green'></span><a href='../sentence_detail/58.htm' target='right'><span class='orange'>虽然基于分布的聚类方法有着优秀的理论基础,</span></a><span class='green'>但是它们却容易导致过拟合问题。因此,我们需要对这类模型添加复杂性约束。从理论上而言,</span><a href='../sentence_detail/59.htm' target='right'><span class='orange'>选择越复杂的模型越能更好地约束数据,</span></a><span class='green'>但是选择合适的复杂度模型却是十分困难的,</span><a href='../sentence_detail/60.htm' target='right'><span class='orange'>最常用的模型是高斯混合模型(使用期望最大化算法)。</span></a><span class='green'></span><a href='../sentence_detail/61.htm' target='right'><span class='orange'>高斯混合模型是以数据服从高斯混合分布为假设的,换言之,数据可以看作是从多个高斯分布随机选择出来的。</span></a><span class='green'></span><a href='../sentence_detail/62.htm' target='right'><span class='orange'>相对于K-Means算法,高斯混合模型每一次的迭代计算量均较大。</span></a><span class='green'>由于高斯混合模型的聚类方法来源于EM算法,因此可能会产生局部极值,这与初始参数的选取密切相关。</span><a href='../sentence_detail/63.htm' target='right'><span class='orange'>高斯混合模型不仅可以用于聚类分析,同样也可用于概率密度估计。</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>23</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>④基于密度的聚类</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>24</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/64.htm' target='right'><span class='orange'>基于密度的聚类分析算法的主要目是搜寻被低密度区域分割的高密度区域。</span></a><span class='green'></span><a href='../sentence_detail/65.htm' target='right'><span class='orange'>不同于基于距离的聚类算法(基于距离的聚类算法是以数据集为球状簇为前提进行聚类的),基于密度的聚类算法可以发现任意形状的簇,这有利于处理带有噪声点的数据。</span></a><span class='green'></span><a href='../sentence_detail/66.htm' target='right'><span class='orange'>在基于密度的聚类分析算法中,</span></a><span class='green'></span><a href='../sentence_detail/67.htm' target='right'><span class='orange'>分布在稀疏区域的对象通常被认为是噪声或边界点。</span></a><span class='green'>目前,</span><a href='../sentence_detail/68.htm' target='right'><span class='red'>最流行的基于密度的聚类算法是DBSCAN(Density-Based Spatial Clustering of Application with Noise)[7]。</span></a><span class='green'></span><a href='../sentence_detail/69.htm' target='right'><span class='orange'>OPTICS[8]是DBSCAN的变形,</span></a><span class='green'>它并不需要为范围参数选择合适的值就可以产生与分层聚类相似的分层效果。Density-Link-Clustering结合单连通聚类和OPTICS的思想,完全消除了参数,并且通过使用R树索引增强了聚类性能。</span><a href='../sentence_detail/70.htm' target='right'><span class='orange'>DBSCAN和OPTICS的主要缺点是它们是通过某种程度的密度下降来检测簇边界的。</span></a><span class='green'></span><a href='../sentence_detail/71.htm' target='right'><span class='orange'>此外,它们无法检测现实生活数据中普遍存在的内在簇结构。</span></a><span class='green'>而DBSCAN的变形方法EnDBSCAN可以解决此类问题[9]。Mean-shift方法基于核密度估计[10],将每个对象移动到其附近最密集的区域。最终,对象会收敛到密度的局部最大值。但是由于昂贵的迭代过程和密度估计,Mean-shift通常比DBSCAN的效率要低。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>25</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>图 1.2 聚类分析算法分类</span><br><span class='green'>Figure 1.2 The classification of clustering analysis</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>26</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>近年来,诸多学者投身于提高现有算法性能的研究中[12][13],</span><a href='../sentence_detail/72.htm' target='right'><span class='orange'>其中包括CLARANS(Ng和Han,1994)[14][18]和BIRCH(Zhang等,1996)[15]。</span></a><span class='green'>由于处理海量数据集的需求日益增长,所以研究人员试图通过换取聚类性能以增加所产生簇的语义分析能力,这一意愿引起了pre-clustering的发展,其中以canopy聚类最具代表性[16]。Canopy聚类算法可以处理超大型数据集,但是所得到的“聚类”仅仅是对数据集的粗略预分割,之后仍需使用现有的聚类分析算法对这些预分割数据集进行聚类。学者们一直在进行各种各样的聚类算法尝试,比如基于seed的聚类方法[17]。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>27</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>本文研究的主要内容</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>28</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/73.htm' target='right'><span class='orange'>从聚类分析的国内外现状可以看出,</span></a><span class='green'>基于划分的聚类算法适用于凸集或者类球形的数据集;</span><a href='../sentence_detail/74.htm' target='right'><span class='orange'>基于密度的聚类算法依赖于数据集的密度分布,</span></a><span class='green'></span><a href='../sentence_detail/75.htm' target='right'><span class='orange'>因此可以发现任意形状的簇,适用范围更加广泛。</span></a><span class='green'>但是高维数据空间的稀疏性和距离集中使得无论是基于距离还是密度的聚类算法都变得不再适用。因此,本文引进了hubness这一概念,并利用逆近邻的偏度很好地解决了这一问题。针对传统的聚类分析算法所存在的问题,</span><a href='../sentence_detail/76.htm' target='right'><span class='orange'>本文主要完成了以下几方面的工作:</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>29</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/77.htm' target='right'><span class='orange'>第一、研究了课题的相关背景及其意义,</span></a><span class='green'>从数据挖掘的实践应用和理论基础两方面对聚类分析的国内外现状进行了概述总结,着重阐述了聚类分析在高维数据中所遇到的挑战。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>30</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>第二、通过查看相关的聚类分析算法文献,</span><a href='../sentence_detail/78.htm' target='right'><span class='orange'>例如层次聚类算法、划分聚类算法、基于分布的聚类算法以及基于密度的聚类算法等,</span></a><span class='green'>对以上算法有了一定的了解并总结出了算法各自的优缺点及其适用范围。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>31</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/79.htm' target='right'><span class='orange'>第三、比较了现有的聚类分析算法,</span></a><span class='green'></span><a href='../sentence_detail/80.htm' target='right'><span class='orange'>如K-Means,DBSCAN等,</span></a><span class='green'>并且针对其基本思想进行了分析研究。研究表明,无论是基于距离的聚类算法还是基于密度的聚类算法都无法解决维数灾难的问题,同时研究了可在高维数据空间聚类的hub聚类算法。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>32</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>第四、将原有的hub聚类分析算法与主成分分析相结合,提出“无损”降维聚类的PCA-Hub聚类算法,该算法解决了高维数据中存在的冗余和噪声数据,</span><a href='../sentence_detail/81.htm' target='right'><span class='orange'>同时在不损失重要的有价值信息的情况下对数据集进行降维,</span></a><span class='green'>增强了算法的聚类性能。PCA-Hub聚类算法虽然可以很好地解决高维数据空间中的冗余和噪声数据,</span><a href='../sentence_detail/82.htm' target='right'><span class='orange'>然而随着数据集尺度和数据集维数的不断增加,</span></a><span class='green'>PCA-Hub聚类算法的耗时将会变得越来越严重甚至是不可接受。因此,本文提出了一种Quick PCA-Hub聚类分析算法分别从快速搜索k个主成分和快速搜索最近邻居两方面加快PCA-Hub算法的聚类分析速度。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>33</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>第五、本文采用若干个UCI数据集,将PCA-Hub聚类算法与传统的K-Means算法和hub聚类算法进行对比实验,揭示了无论数据集是否呈现出较高的hubness情况下该算法均可以取得不错的聚类效果。若数据集未呈现出较高的hubness现象时,传统的K-Means方法更为适用;然而,当数据集表现出较高的hubness现象时,hub聚类算法则会取得不错的聚类效果。Quick PCA-Hub聚类算法相比之前的聚类算法在轮廓系数上平均提高了8%,在高维数据空间中搜索理想的k个主成分时相比PCA-Hub聚类算法表现出了巨大的优势。根据对比实验结果揭示了该算法的聚类效果相对较佳,同时给出了详细的实验结果分析,以及深入讨论了各算法的适用性和优缺点。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>34</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>论文的章节排版</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>35</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>本文大致分为5大章节,详细的论文排版结构如下:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>36</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>第1章 绪论:主要概述了数据挖掘领域的研究背景及其意义,着重分析了传统聚类算法和hub聚类算法,</span><a href='../sentence_detail/83.htm' target='right'><span class='orange'>同时阐述了聚类分析的现实意义和价值。</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>37</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>第2章 聚类分析概述:介绍了当前主流的几大经典聚类算法,着重研究了基于距离的划分聚类方法和基于密度的聚类分析方法,并且列出了各类算法的适用性和优缺点。最后,对当前聚类分析的发展趋势和热点问题进行了简要地概述和总结。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>38</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>第3章 PCA-Hub聚类算法:详细地介绍了hubness这一现象的起源、定义以及hub聚类分析算法,</span><a href='../sentence_detail/84.htm' target='right'><span class='orange'>通过实验对hub聚类算法进行了深入的分析研究,并归纳出了其适用性及优缺点。</span></a><span class='green'>由于hub聚类算法未处理高维数据空间中的冗余和噪声数据从而无法获得更优的簇结构以及更快的聚类收敛速度,因此本文提出了PCA-Hub的聚类算法用于解决此问题。PCA-Hub的聚类算法是以逆近邻偏度的变化率作为降维标准,在降维的同时保留了大量有价值信息从而提高了聚类效果。</span><a href='../sentence_detail/85.htm' target='right'><span class='orange'>在UCI的若干个数据集上进行实验分析,</span></a><span class='green'>将该算法与K-Means算法以及hub聚类算法等进行实验对比分析,并通过轮廓系数作为聚类结果的评价指标。实验结果表明,PCA-Hub聚类算法相比之前的聚类算法在轮廓系数上平均提高了15%;当数据集的维数或者k-occurrences的偏度较高时,PCA-Hub聚类算法对近邻数的选择不敏感;在实验环境和实验参数一致的情况下,PCA-Hub聚类算法的结果在很大程度上具有一致性。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>39</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>第4张 Quick PCA-Hub聚类算法:PCA-Hub聚类算法虽然可以很好地解决高维数据空间中的冗余和噪声数据,</span><a href='../sentence_detail/86.htm' target='right'><span class='orange'>然而随着数据集尺度和数据集维数的不断增加,</span></a><span class='green'>PCA-Hub聚类算法的耗时将会变得越来越严重甚至是不可接受。因此,本文提出了一种Quick PCA-Hub聚类分析算法分别从快速搜索k个主成分和快速搜索最近邻居两方面加快PCA-Hub算法的聚类分析速度。实验结果表明,Quick PCA-Hub聚类算法相比之前的聚类算法在轮廓系数上平均提高了8%;Quick PCA-Hub在高维数据空间中搜索理想的k个主成分时表现出了巨大的优势。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>40</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/87.htm' target='right'><span class='orange'>第5章 总结与展望:主要对本文中的重点工作进行了概括总结,同时指出研究过程中存在的不足,</span></a><span class='green'>并且对将来的工作以及研究重点作了简要的说明。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>41</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>1绪 论</span><br><span class='green'>1绪 论</span><br><span class='green'>49</span><br><span class='green'>22</span><br><span class='green'>25</span><br><span class='green'>聚类分析概述</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>42</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/88.htm' target='right'><span class='red'>聚类分析(Cluster analysis),也称为群集分析,</span></a><span class='green'>常用于统计数据分析,在诸多领域均拥有广泛应用。</span><a href='../sentence_detail/89.htm' target='right'><span class='orange'>聚类是将元素分成不同的组别或者更多的子集,</span></a><span class='green'>使得分配到相同簇中的元素彼此之间比其它的数据点更为相似,也就是说,聚类算法的目的是要增加类内的相似性并减小类间的相似性。</span><a href='../sentence_detail/90.htm' target='right'><span class='orange'>聚类分析不同于分类,分类常被视为是有监督的学习,</span></a><span class='green'></span><a href='../sentence_detail/91.htm' target='right'><span class='orange'>而聚类分析一般归纳为一种无监督式的学习。</span></a><span class='green'>聚类分析不依赖于先验知识(类标签),只依赖自身属性,通过自身属性可以区分簇之间的相似性或者对象之间的相似性。</span><a href='../sentence_detail/92.htm' target='right'><span class='orange'>聚类分析作为一门十分有效的数据分析技术,</span></a><span class='green'></span><a href='../sentence_detail/93.htm' target='right'><span class='orange'>常被应用于机器学习、数据挖掘、模式识别以及生物信息等领域。</span></a><span class='green'>本章将详尽地描述各种聚类算法,根据归纳的文献综述,</span><a href='../sentence_detail/94.htm' target='right'><span class='orange'>聚类分析算法大致可以分为以下四种:</span></a><span class='green'></span><a href='../sentence_detail/95.htm' target='right'><span class='orange'>层次聚类算法、基于中心的聚类算法、基于分布的聚类算法以及基于密度的聚类算法。</span></a><span class='green'></span><a href='../sentence_detail/96.htm' target='right'><span class='orange'>本章主要阐述了各类聚类算法的基本思想,并总结归纳出了各自的优缺点和适用范围。</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>43</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>聚类分析的定义</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>44</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>由于难以对簇的概念作出准确定义,从而导致有诸多的聚类算法产生[19]。虽然不同的聚类算法对簇的概念定义不同,但是它们却有却有一个共同之处:簇是一组数据对象。不同聚类分析算法使用了不同的聚类模型,掌握聚类模型是了解聚类分析算法的关键,下面仅列出了主流的聚类模型:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>45</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>①连通性模型(Connectivity Models),例如层次聚类基于距离连通性构建模型;</span><br><span class='green'>②中心性模型(Centroid Models),例如K-Means算法将单个平均向量表示每个簇类;</span><br><span class='green'>③分布模型(Distribution Models),使用统计分布对聚类进行建模,例如由EM算法使用的是多变量正态分布;</span><br><span class='green'>④密度模型(Density Models),例如DBSCAN和OPTICS将簇定义为数据空间中的连接密集区域;</span><br><span class='green'>⑤子空间模型(Subspace Models),在Biclustering(也称为协同聚类或双模式聚类)中使用群集成员和相关属性建模;</span><br><span class='green'>⑥组模型(Group Models),一些算法不提供精确模型,仅提供分组信息;</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>46</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>⑦基于图论的模型(Graph-Based Models):图中的节点分成了若干个子集,在这些子集中的每两个点都通过一条边相连。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>47</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>一般而言,</span><a href='../sentence_detail/97.htm' target='right'><span class='orange'>不同的聚类模型对应着不同的聚类分析算法。</span></a><span class='green'>但是从本质上来说,聚类分析就是一组簇的集合,</span><a href='../sentence_detail/98.htm' target='right'><span class='orange'>该集合通常包含数据集的所有对象,</span></a><span class='green'>因此聚类分析可以大致地分为以下两种:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>48</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>①硬聚类(hard clustering):每个数据对象要么属于一个簇,要么不属于任何簇;</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>49</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>②软聚类(soft clustering,也称为模糊聚类fuzzy clustering):每个数据对象有一定的概率属于每个簇。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>50</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>如果需要进一步对聚类进行划分,可参照如下严格的聚类划分结果:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>51</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>①严格划分聚类,每个对象正好属于一个簇;</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>52</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/99.htm' target='right'><span class='orange'>②包含离群点的严格划分聚类,</span></a><span class='green'></span><a href='../sentence_detail/100.htm' target='right'><span class='orange'>对象也可以不属于任何簇,那么它将会被视为离群点;</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>53</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/101.htm' target='right'><span class='orange'>③重叠聚类(也称作可替代聚类或多视图聚类),虽然通常是硬聚类,但对象也可能属于多个簇;</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>54</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>④分层聚类:属于子集群的对象同时也属于父集群;</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>55</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/102.htm' target='right'><span class='orange'>⑤子空间聚类:在唯一定义的子空间内尽可能的没有重叠簇。</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>56</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>聚类分析算法是根据它们的聚类模型进行分类的,没有客观的“正确的”聚类算法,正如Vladimir已经指出的,“聚类是在旁观者的眼中(Clustering is in the eye of the beholder)”。针对特定的问题,除非有数据理论依据,否则需要通过实验进行选择合适的聚类算法[19]。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>57</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>常用的聚类分析算法</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>58</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>下面仅列出最主流的聚类算法。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>59</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>层次聚类算法</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>60</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/103.htm' target='right'><span class='orange'>层次聚类算法也称作基于连通性的聚类算法,</span></a><span class='green'></span><a href='../sentence_detail/104.htm' target='right'><span class='orange'>其核心思想是若两个对象越接近,</span></a><span class='green'>那么它们的相关性就越强。这些算法基于对象间的距离将彼此连通从而形成不同的簇。在很大程度上,一个簇可以由该簇内的最大连通距离来表示。不同的距离会形成不同的簇,这可以通过树形结构来表示,这也是层次聚类名称的来源。</span><a href='../sentence_detail/105.htm' target='right'><span class='orange'>层次聚类可分为两大类:自底向上(agglomerative)和自顶向下(divisive)。</span></a><span class='green'></span><a href='../sentence_detail/106.htm' target='right'><span class='orange'>自底向上是开始时所有数据点均各自为一个类别,然后每次迭代将距离最近的两个类合并,直到只有一个类为止。</span></a><span class='green'>自顶向下的思想与自底向上的思想正好完全相反。</span><a href='../sentence_detail/107.htm' target='right'><span class='orange'>计算两个类之间的距离共有三种方法:</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>61</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/108.htm' target='right'><span class='orange'>①Single Linkage(也称作nearest-neighbor),是指类之间的距离为这两个类中距离最近的两个点之间的距离,然而这容易导致“Chaining”现象的发生。</span></a><span class='green'>“Chaining”现象是指原本整体相距较远的簇只因其中个别点之间的距离较近而被合并,若依此合并最终会得到比较松散的簇;</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>62</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/109.htm' target='right'><span class='orange'>②Complete Linkage:是Single Linkage的反面极端,是指类之间的距离为这两个类中距离最远的两个点之间的距离。</span></a><span class='green'>负面效果显而易见,原本已经很近的两个簇,只因有不配合的点存在而不能合并。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>63</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/110.htm' target='right'><span class='red'>③Group Average:</span></a><span class='green'>是指把类之间的距离定义为所有点对的距离的平均值。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>64</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/111.htm' target='right'><span class='orange'>虽然层次聚类的核心思想比较简单,但是计算复杂度却比较高。</span></a><span class='green'>因为上述的三种方法均需要计算所有点对之间的距离,</span><a href='../sentence_detail/112.htm' target='right'><span class='orange'>而且算法也表明每次迭代只能合并两个子类,这是非常耗时的。</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>65</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>基于中心的聚类算法</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>66</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/113.htm' target='right'><span class='orange'>在基于质心的聚类中,由中心向量表示一个簇,</span></a><span class='green'>该簇的中心元素不一定是数据集中的元素。当簇的个数为定值k时,K-Means聚类方法给出了关于最优化问题的形式定义:搜寻k个簇中心,然后将对象划分给与之最近的簇中心所在的簇。K-Means的目的是最小化簇内平方和(WCSS within-cluster sum of squares)。</span><a href='../sentence_detail/114.htm' target='right'><span class='orange'>最优化问题本身是一个NP难问题,</span></a><span class='green'></span><a href='../sentence_detail/115.htm' target='right'><span class='orange'>因此常见的方法是找到其近似解。</span></a><span class='green'>最著名的近似方法是Lloyd's[d5],也就是K-Means算法。然而,</span><a href='../sentence_detail/116.htm' target='right'><span class='orange'>K-Means算法智能只能找到局部最优值,</span></a><span class='green'>而且使用的是随机的初始化值。</span><a href='../sentence_detail/117.htm' target='right'><span class='orange'>K-Means的衍生算法作了如下的改进:</span></a><span class='green'>选择多次运行的最优值,并且将簇中心限定为数据集中的元素(K-Medoids);选择中值作为簇中心(K-Medians);较少的选择随机值作为簇中心(K-Means++);</span><a href='../sentence_detail/118.htm' target='right'><span class='orange'>或者可以模糊聚类(Fuzzy C-Means)。</span></a><span class='green'>大多数基于K-Means算法的最大缺点之一是需要预先指定簇的个数k。此外,由于这些算法均是基于簇中心的,所以更容易形成大小相似的簇,这通常导致簇之间不正确的边界切割。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>67</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>K-Means有以下重要的理论性质。首先,</span><a href='../sentence_detail/119.htm' target='right'><span class='orange'>它可以将数据划分为Voronoi图结构。</span></a><span class='green'>其次,在理论上它与最近邻概念接近,因此在机器学习领域大受欢迎。</span><a href='../sentence_detail/120.htm' target='right'><span class='orange'>最后,它可以被视为基于模型分类的变形,</span></a><span class='green'></span><a href='../sentence_detail/121.htm' target='right'><span class='orange'>并且K-Means算法是EM算法的一种变形。</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>68</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>基于分布的聚类算法</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>69</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>与统计学最密切相关的聚类模型是基于分布的模型。对象的分布越相似,它们分配在同一个簇中的可能性越大。</span><a href='../sentence_detail/122.htm' target='right'><span class='orange'>该方法易于处理类似于人工生成的数据集(从分布中随机取样)。</span></a><span class='green'></span><a href='../sentence_detail/123.htm' target='right'><span class='orange'>虽然基于分布的聚类方法有着优秀的理论基础,</span></a><span class='green'>但是它们却容易导致过拟合问题。因此,我们需要对这类模型添加复杂性约束。从理论上而言,</span><a href='../sentence_detail/124.htm' target='right'><span class='orange'>选择越复杂的模型越能更好地约束数据,</span></a><span class='green'>然而选择合适的复杂度模型却是十分困难的。最常用的基于分布的聚类算法的模型是高斯混合模型(使用期望最大化算法)。</span><a href='../sentence_detail/125.htm' target='right'><span class='orange'>高斯混合模型是以数据服从高斯混合分布为假设的,换言之,数据可以看作是从多个高斯分布随机选择出来的。</span></a><span class='green'></span><a href='../sentence_detail/126.htm' target='right'><span class='orange'>每个高斯混合模型由k个高斯分布组成,每个高斯分布被称作一个"Component",</span></a><span class='green'>高斯混合模型的概率密度函数是由这些Component线性组合而成,其公式如下:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>70</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>(2.1)</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>71</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/127.htm' target='right'><span class='red'>其中K为模型的个数,为第k个高斯的权重,为第k个高斯的概率密度函数,其均值为,方差为。</span></a><span class='green'></span><a href='../sentence_detail/128.htm' target='right'><span class='orange'>此概率密度的估计就是要计算、和各个变量。</span></a><span class='green'></span><a href='../sentence_detail/129.htm' target='right'><span class='red'>当求出的表达式后,求和式的各项的结果就分别代表样本x属于各个类的概率。</span></a><span class='green'>高斯混合模型的优点是投影后的样本点将获得每个类的概率,而非一个确切的分类标签。</span><a href='../sentence_detail/130.htm' target='right'><span class='orange'>相对于K-Means算法,高斯混合模型每一次的迭代计算量都比较大。</span></a><span class='green'>由于高斯混合模型的聚类方法来源于EM算法,因此可能会产生局部极值,这与初始参数的选取密切相关。</span><a href='../sentence_detail/131.htm' target='right'><span class='orange'>高斯混合模型不仅可以用于聚类分析,同样也可用于概率密度估计。</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>72</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>基于密度的聚类算法</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>73</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/132.htm' target='right'><span class='orange'>基于密度的聚类分析算法的主要目是搜寻被低密度区域分割的高密度区域。</span></a><span class='green'></span><a href='../sentence_detail/133.htm' target='right'><span class='orange'>不同于基于距离的聚类算法(基于距离的聚类算法是以数据集是球状簇为前提进行聚类的),基于密度的聚类算法可以发现任意形状的簇,这有利于处理带有噪声点的数据。</span></a><span class='green'>在基于密度的聚类算法中,</span><a href='../sentence_detail/134.htm' target='right'><span class='orange'>分布在稀疏区域的对象通常被认为是噪声或边界点。</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>74</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>目前,</span><a href='../sentence_detail/135.htm' target='right'><span class='red'>最流行的基于密度的聚类算法是DBSCAN(Density-Based Spatial Clustering of Application with Noise)[7]。</span></a><span class='green'>在DBSCAN算法中数据点可以分为以下三类:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>75</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/136.htm' target='right'><span class='orange'>①核心点:在-邻域内含有超过MinPts数目的点;</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>76</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>②边界点:</span><a href='../sentence_detail/137.htm' target='right'><span class='red'>在-邻域内点的数量小于MinPts,但是落在核心点的邻域内;</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>77</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>③噪音点:</span><a href='../sentence_detail/138.htm' target='right'><span class='red'>既不是核心点也不是边界点的点。</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>78</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>其中,为邻域半径,MinPts为指定的数目。</span><a href='../sentence_detail/139.htm' target='right'><span class='orange'>DBSCAN的算法思想十分简单:</span></a><span class='green'></span><a href='../sentence_detail/140.htm' target='right'><span class='red'>若一个点p的-邻域包含多于MinPts个对象,那么创建以p作为核心对象的新簇;</span></a><span class='green'></span><a href='../sentence_detail/141.htm' target='right'><span class='orange'>搜寻与核心对象直接密度可达的对象,将其合并;若没有新的点可以更新簇时,算法结束。</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>79</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/142.htm' target='right'><span class='orange'>OPTICS是DBSCAN的变形[8],</span></a><span class='green'>它并不需要为范围参数选择合适的值,就产生与分层聚类相似的分层效果。Density-Link-Clustering结合单连通聚类和OPTICS的思想,完全消除了参数,并且通过使用R树索引增强了聚类性能。</span><a href='../sentence_detail/143.htm' target='right'><span class='orange'>DBSCAN和OPTICS的主要缺点是它们是通过某种程度的密度下降来检测簇边界的。</span></a><span class='green'></span><a href='../sentence_detail/144.htm' target='right'><span class='orange'>此外,它们无法检测现实生活数据中普遍存在的内在簇结构。</span></a><span class='green'>而DBSCAN的变形方法EnDBSCAN可以解决此类问题[9]。Mean-shift方法基于核密度估计[10],将每个对象移动到其附近最密集的区域。最终,对象会收敛到密度的局部最大值。由于昂贵的迭代过程和密度估计,Mean-shift通常比DBSCAN的效率要低</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>80</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>表 2.1聚类分析算法对比表</span><br><span class='green'>Table 2.1 Clustering analysis algorithm comparison table</span><br><span class='green'>算法名称</span><br><span class='green'>参数</span><br><span class='green'>可伸缩型</span><br><span class='green'>用例</span><br><span class='green'>几何结构(度量标准)</span><br><span class='green'>K-Means</span><br><span class='green'>簇的个数</span><br><span class='green'>非常大的样本数,中等簇的个数</span><br><span class='green'>一般用途,簇的规模大致相等,平面几何,簇的数量不可过多</span><br><span class='green'>点对之间的距离</span><br><span class='green'>Affinity propagation</span><br><span class='green'>阻尼,样本首选项</span><br><span class='green'>在样本数方面不具有伸缩性</span><br><span class='green'>簇的数量较多,簇的规模有明显差异, 非平面几何</span><br><span class='green'>图距离</span><br><span class='green'>Mean-shift</span><br><span class='green'>带宽</span><br><span class='green'>在样本数方面不具有伸缩性</span><br><span class='green'>簇的数量较多,簇的规模有明显差异, 非平面几何</span><br><span class='green'>点对之间的距离</span><br><span class='green'>Spectral clustering</span><br><span class='green'>簇的个数</span><br><span class='green'>中等样本数,小型簇的个数</span><br><span class='green'>簇的数量不可过多,簇的规模大致相等,非平面几何</span><br><span class='green'>图距离</span><br><span class='green'>Ward hierarchical clustering</span><br><span class='green'>簇的个数</span><br><span class='green'>大型样本数和簇的个数</span><br><span class='green'>簇的数量较多,可包含连通性约束条件</span><br><span class='green'>点对之间的距离</span><br><span class='green'>Agglomerative clustering</span><br><span class='green'>簇的个数,连通类型,距离</span><br><span class='green'>大型样本数和簇的个数</span><br><span class='green'>簇的数量较多,可包含连通性约束条件,非欧氏距离</span><br><span class='green'>任意点对之间的距离</span><br><span class='green'>DBSCAN</span><br><span class='green'>邻域大小</span><br><span class='green'>非常大的样本数,中等簇的个数</span><br><span class='green'>簇的规模有明显差异, 非平面几何</span><br><span class='green'>最近点之间的距离</span><br><span class='green'>Gaussian mixtures</span><br><span class='green'>参数较多</span><br><span class='green'>不具有可伸缩型</span><br><span class='green'>平面几何,有易于密度估计</span><br><span class='green'>到中心的马哈拉诺比斯距离</span><br><span class='green'>Birch</span><br><span class='green'>分枝, 阈值,可选的全局聚类器</span><br><span class='green'>非常大的样本数,中等簇的个数</span><br><span class='green'>大型数据集,离群点可移除,可数据简化</span><br><span class='green'>点对之间的欧式距离</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>81</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>近年来,诸多学者投身于提高现有算法性能的研究中[12][13],</span><a href='../sentence_detail/145.htm' target='right'><span class='orange'>其中包括CLARANS(Ng和Han,1994)[14][18]和BIRCH(Zhang等,1996)[15]。</span></a><span class='green'>由于处理海量数据集的需求日益增长,所以研究人员试图通过换取聚类性能以增加所产生簇的语义分析能力,这一意愿引起了pre-clustering的发展,其中以canopy聚类最具代表性[16]。Canopy聚类算法可以处理超大型数据集,但是所得到的“聚类”仅仅是对数据集的粗略预分割,之后仍需使用现有的聚类分析算法对这些预分割数据集进行聚类。学者们一直在进行各种各样的聚类算法尝试,比如基于seed的聚类方法[17]。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>82</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>聚类分析的评价标准</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>83</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/146.htm' target='right'><span class='orange'>每种聚类算法都有其各自的适用范围,</span></a><span class='green'>有的适用于小型数据集,</span><a href='../sentence_detail/147.htm' target='right'><span class='orange'>有的可以处理大型数据,有的可以发现任意形状的簇。</span></a><span class='green'>但总的来说,数据挖掘针对聚类分析算法有以下评价标准:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>84</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>①处理大型数据的能力</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>85</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>科学技术的发展使得获取的数据集的规模越来越大,那么能够处理大型数据集的能力已成为聚类分析算法的一般要求。目前,主流的聚类算法处理大型数据的主要方法通常是对大型数据进行随机取样获得一个较小型的数据集,然后再用聚类算法对样本数据集进行分析。</span><a href='../sentence_detail/148.htm' target='right'><span class='orange'>但是,这种方法有一个明显的缺点,</span></a><span class='green'></span><a href='../sentence_detail/149.htm' target='right'><span class='orange'>样本数据集可能会导致不同的聚类结果,</span></a><span class='green'>难以对整体数据集进行真实客观的分析。</span><a href='../sentence_detail/150.htm' target='right'><span class='orange'>所以聚类算法的可伸缩性是现如今一个十分重要的研究内容。</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>86</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>②处理不同类型属性的能力</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>87</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>聚类分析算法不应该只能处理单一的数据类型,</span><a href='../sentence_detail/151.htm' target='right'><span class='orange'>而应该可以处理不同的数据类型,例如二元类型数据、分类/标称类型数据、序数型数据等。</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>88</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>③可以发现任意形状的簇:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>89</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>基于距离的聚类算法通常是通过区分对象之间的相似度进行聚类分析的,这类聚类算法趋向于发现数据规模相同或者类球形的簇。然而,现实生活中数据集分布通常是任意形状的。因此,</span><a href='../sentence_detail/152.htm' target='right'><span class='orange'>聚类算法应该具有对不同类型和不同密度的数据集的处理能力。</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>90</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>④初始化参数的数量最小化</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>91</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>聚类分析算法的参数选择一直是一个备受关注的问题,目前很多主流的聚类算法需要预先设置一些输入参数,例如生成簇的数量、近邻数、近邻半径等参数,才能对数据集进行聚类分析。通常来说,这些参数的选取在很大程度上影响着聚类结果的好坏。</span><a href='../sentence_detail/153.htm' target='right'><span class='orange'>如果聚类算法对参数选择十分敏感,</span></a><span class='green'>那么聚类结果的准确性将变得不稳定。因此,聚类算法应该减小参数的设置和参数对聚类结果的影响。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>92</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>⑤处理异常数据的能力</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>93</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>异常数据通常是离群检测中的噪声数据或离群点,在离群检测中通常被视为是远离数据集中绝大多数数据对象的异常数据。现实生活中的数据集普遍存在着异常数据,数据集的噪声和冗余数据通常会使得聚类结果产生较大的差异。因此,能够处理异常数据变得十分重要。目前,一些算法对异常数据具有良好的处理能力,</span><a href='../sentence_detail/154.htm' target='right'><span class='orange'>例如DBSCAN聚类算法和基于密度的CURE聚类算法;</span></a><span class='green'>然而一些传统的聚类算法并不具备处理异常数据的能力,</span><a href='../sentence_detail/155.htm' target='right'><span class='orange'>例如ANGES聚类算法和K-Means聚类算法。</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>94</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>⑥处理高维数据的能力</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>95</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/156.htm' target='right'><span class='red'>现在,随着数据集维数的不断增加,</span></a><span class='green'>聚类算法处理高维数据的能力也变得越来越重要。同时,由于高维数据的稀疏性和距离集中使得一些算法的聚类结果相较低维数据变得很差。虽然目前已提出一些处理高维数据的聚类算法,例如CLIQUE聚类算法,但是由于其聚类结果差异较大,所以在高维数据空间中CLIQUE聚类算法的应用并不广泛。</span><a href='../sentence_detail/157.htm' target='right'><span class='orange'>近年来,越来越多的研究人员开始关注高维数据,</span></a><span class='green'></span><a href='../sentence_detail/158.htm' target='right'><span class='orange'>同时在高维数据空间中的聚类分析已成为目前聚类分析的一个主要方向。</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>96</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>⑦可解释性和可用性</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>97</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/159.htm' target='right'><span class='orange'>聚类分析是一种非监督式的机器学习方法,</span></a><span class='green'>对于要处理的数据集无法预先知道其统计分布情况,</span><a href='../sentence_detail/160.htm' target='right'><span class='orange'>所以当对数据集进行聚类分析后,</span></a><span class='green'>能够根据结果进行合理地解释,分析出数据的内在规律和模式。在低维数据空间中,例如2维或3维空间,可以通过绘图简单直观地展示聚类结果。然而在高维数据空间中,</span><a href='../sentence_detail/161.htm' target='right'><span class='orange'>如何解释聚类结果已成为目前聚类分析的一个主要方向。</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>98</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>聚类分析的评估检验</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>99</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>聚类结果的评估也被称为聚类验证。两个簇之间的相似性有诸多的检测方法。这些方法可以衡量不同的聚类方法对同一数据集的聚类效果。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>100</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>内部检验</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>101</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>内部评估是指聚类结果的评估依赖于聚类的本身数据。</span><a href='../sentence_detail/162.htm' target='right'><span class='orange'>当聚类结果表现出高的类内相似性和低的类间相似性时,</span></a><span class='green'>这些评估方法会给出一个较高的分值。然而,具有高分值的内部评估却并不一定能够进行有效地信息检索[20]。另外,内部评估容易倾向于使用相同聚类模型的算法。比如,基于最优化对象间距离的K-Means算法,同样基于距离的内部评估将可能高估得到的聚类结果。因此,内部评估方法适用于比较两种算法的性能优劣,然而这却不包含有效性结果(valid results)的比较[19]。有效性指标依赖于数据集本身的结构。</span><a href='../sentence_detail/163.htm' target='right'><span class='orange'>比如,K-Means算法只能找到凸簇,</span></a><span class='green'>所以许多评估指标都是以此为假设的。同样,在具有非凸簇的数据集上,</span><a href='../sentence_detail/164.htm' target='right'><span class='orange'>既不使用K-Means 算法,</span></a><span class='green'>也不采用假定凸簇的评估标准。以下是基于内部标准的评估方法:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>102</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>①Davies–Bouldin指数</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>103</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>Davies-Bouldin指数计算公式如下:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>104</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>(2.2)</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>105</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/165.htm' target='right'><span class='orange'>其中,n为簇的个数,是簇的质心,</span></a><span class='green'>是簇中所有元素到簇质心的距离的平均值,是簇和簇之间的距离。因此,具有越低的Davies–Bouldin 指数则表明聚类的结果越好。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>106</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>②Dunn指数</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>107</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>Dunn指数的目标是识别具有高密度且良好分割的簇群,</span><a href='../sentence_detail/166.htm' target='right'><span class='orange'>它是最小化簇间距离与最大化簇内距离的比值。</span></a><span class='green'></span><a href='../sentence_detail/167.htm' target='right'><span class='orange'>Dunn指数的计算公式如下:</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>108</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>(2.3)</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>109</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/168.htm' target='right'><span class='orange'>其中,表示簇i和簇j之间的距离,</span></a><span class='green'>为簇k的内距离。簇间距可以任意选择一种度量方式,比如,假定两个簇的中心之间的距离为两个簇之间的距离。同样,簇内距也有多重表示方式,比如,假定簇内的任意点对之间的最大距离为簇的簇内距。因此,具有越高的Dunn指数值则表明聚类的结果越好。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>110</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>③轮廓系数</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>111</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>轮廓系数是簇内点对之间的平均距离与该簇内的点到其它簇的距离的最大值的比值[48],其计算公式如下所示:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>112</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>(2.4)</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>113</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>其中,表示i向量到同一簇内其他点不相似程度的平均值,表示i向量到其他簇的平均不相似程度的最小值。可见轮廓系数的值总是介于[-1,1],越趋近于1代表内聚度和分离度都相对较优。</span><a href='../sentence_detail/169.htm' target='right'><span class='red'>将所有点的轮廓系数求平均,就是该聚类结果总的轮廓系数。</span></a><span class='green'>轮廓系数适用于K-Means算法,也可用于确定最优的聚类数。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>114</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>外部检验</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>115</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>在外部检验中,</span><a href='../sentence_detail/170.htm' target='right'><span class='orange'>聚类结果的评估依赖于未进行聚类的数据,</span></a><span class='green'>例如已知的类标签和外部基准(external benchmark)。这些基准通常是由该方面的专家设置的一组预分类的元素。因此,这些基准集通常被视为是检验的黄金标准。这些检验方法用于比较聚类结果与预定基准类之间的近似程度。</span><a href='../sentence_detail/171.htm' target='right'><span class='orange'>由于类可能包含内部结构、属性不允许分离簇以及类可能包含异常情况等等,</span></a><span class='green'>这些因素导致研究人员对基准集是否能够对真实数据进行有效检验产生了疑问[21]。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>116</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>这些方法与评估分类问题的方法相似。不同于统计被正确标记的类,这些方法统计的是同一个簇内点对之间相同标签的个数。以下是基于外部标准的评估方法:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>117</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>①纯度</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>118</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>纯度用于衡量每个簇中包含单一类的个数[20],换言之,纯度是用于统计当前簇中最常见的类的样本点的个数。将所有簇的纯度累加并除以数据集的样本数就是该数据集的纯度。纯度的计算公式如下:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>119</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>(2.5)</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>120</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>其中,M为簇集,D为类标签集,N为数据集的样本数。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>121</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>②Rand指数</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>122</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>Rand用于衡量聚类簇与基准分类信息的相似度[22],也可视为该算法所作出的正确决策的百分比,其计算公式如下:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>123</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>(2.6)</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>124</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>其中,</span><a href='../sentence_detail/172.htm' target='right'><span class='orange'>TP为同一个类的点被分到同一个簇的数量,TN为不同类的点被分到不同簇的数量,FP为不同类的点被分到同一个簇的数量,</span></a><span class='green'>FN为同一类的点被分到不同簇的数量。Rand指数存在的一个问题是FP和FN具有相同的权重。</span><a href='../sentence_detail/173.htm' target='right'><span class='orange'>这对于某些聚类算法而言可能是不期望的特性,</span></a><span class='green'>下面的F-measure会解决这个问题。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>125</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>③F-measure</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>126</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>F-measure通过参数来对召回度进行加权从而平衡FN的分布[23],</span><a href='../sentence_detail/174.htm' target='right'><span class='orange'>精度和召回度的计算公式如下:</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>127</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>(2.7)</span><br><span class='green'>(2.8)</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>128</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>其中P是精度,R是召回度。结合精度和召回度,F-measure的计算公式如下:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>129</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>(2.9)</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>130</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>其中,时,。换言之,当时,召回度对F-measure无影响。随着的增加,召回度在F-measure的权重也在增加。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>131</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>本章小结</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>132</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>本章主要概述了聚类分析的相关信息,首先对聚类分析的定义作了详细说明,</span><a href='../sentence_detail/175.htm' target='right'><span class='orange'>接着介绍了几种主流的聚类模型,</span></a><span class='green'>主要包括连通性模型、中心性模型、分布模型、密度模型以及子空间模型等等;其次,</span><a href='../sentence_detail/176.htm' target='right'><span class='orange'>详细介绍了主流的聚类分析算法,主要包括层次聚类算法、基于中心的聚类算法、基于分布的聚类算法以及基于密度的聚类算法,</span></a><span class='green'>并且列出了代表算法的适用性和优缺点;再次,介绍了聚类分析常用的评价标准;最后,</span><a href='../sentence_detail/177.htm' target='right'><span class='orange'>详细介绍了聚类分析的评估检验方法,</span></a><span class='green'>主要包括内部检验和外部检验。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>133</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>2 聚类分析概述</span><br><span class='green'>PCA-Hub 聚类算法</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>134</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>Hubness这一概念最早是由Milosˇ Radovanovic ́等人在2010年提出的[2],</span><a href='../sentence_detail/178.htm' target='right'><span class='orange'>现已被应用到机器学习、模式识别等领域,</span></a><span class='green'>关于其应用最著名的是解决高维数据空间中的分类问题和聚类问题。Hubness分类问题的思想是给定一个数据集,构建k近邻矩阵,将矩阵中逆近邻数较多的样本点标记为hubs,然后再根据hubs的标签与它的近邻的标签的匹配程度分为good hubs和bad hubs,进而通过某种方式对它的近邻设置相应的权重,接着通过hubs的近邻更新hubs的值,最后对更新后的数据集进行分类处理。Hubness聚类问题的思想是首先构建k近邻矩阵,然后将矩阵中逆近邻数较多的点标记为hubs,接着在聚类迭代的过程中以某种方式将hubs设为当前簇的原型,最后对数据集进行聚类分析。虽然hub聚类算法可以解决传统聚类算法无法处理高维数据的问题,但是由于它未考虑高维数据空间中的冗余和噪声特征,从而降低了聚类性能。因此,本章通过探究逆近邻数的偏度与本征维度的相互关系,在原先的hub聚类算法之上以偏度的变化率为降维依据,保证了在对高维数据降维时不会损失过多的有价值信息,从而提高了聚类效果。本章将对hubness这一现象进行详细的阐述,并深入分析hub聚类算法,再此基础上提出了改进方法。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>135</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>维数灾难</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>136</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/179.htm' target='right'><span class='orange'>维数灾难(Curse of Dimensionality),也称作维度的诅咒,这一术语最初是由Bellman在1961年考虑优化问题时引入的。</span></a><span class='green'>维数灾难用于描述当数据空间的维数增加时,因其数据的体积呈指数型增长而遇到诸多问题的现象,并且该类现象不会出现在低维数据空间中。维数灾难在诸多领域引发了各种各样的问题,这些问题的共同之处在于随着数据维数的增加,</span><a href='../sentence_detail/180.htm' target='right'><span class='orange'>数据的体积将会呈指数型增长,</span></a><span class='green'>从而导致可用数据变得十分稀疏,而数据的稀疏性问题对于任何基于统计学的方法均是一个严峻的挑战。在机器学习领域的挑战中,通过从高维特征空间的有限训练数据中获得某种“自然状态(state of nature)”,</span><a href='../sentence_detail/181.htm' target='right'><span class='orange'>在训练样本的数量恒定时,随着维度的增加其预测能力逐渐减小,这通常称为Hughes影响[24]或者Hughes现象[25][26]。</span></a><span class='green'>在距离度量领域的挑战中,高维数据空间的不同样本之间的距离变得基本相同。令高维欧几里德空间中超球体体积的计算公式为:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>137</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>(3.1)</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>138</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>其中r为超球体的半径,</span><a href='../sentence_detail/182.htm' target='right'><span class='orange'>d为数据集的维数;超立方体的计算公式为:</span></a><span class='green'>。当数据空间的维数趋向于正无穷时,超球体体积与超立方体体积的比值趋向于0,如等式3.2所示:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>139</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>(3.2)</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>140</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>根据等式3.2可以看出,</span><a href='../sentence_detail/183.htm' target='right'><span class='orange'>从某种意义上而言在高维数据空间中几乎所有的数据都远离数据集的中心。</span></a><span class='green'>同时,从另一个角度看通过比较高维数据空间中的最小值距离和最大值距离,可以看出当数据空间的维数趋向于无穷时,最小值距离和最大值距离趋向于相同,从而证明在高维数据空间中距离函数变得不再有意义,公式如下</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>141</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>(3.3)</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>142</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>在最近的研究中,Zimek等人归纳了在搜索高维空间数据时由于维数灾难可能会出现的问题:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>143</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>①分数和距离的集中:用于区分数据样本的相关值(如距离)变得十分相似;</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>144</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/184.htm' target='right'><span class='orange'>②不相关的属性:在高维数据空间中大量的属性可能是不相关的;</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>145</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>③参考集的定义:对于局部方法,参考集通常是基于最近邻的;</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>146</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>④无可比性的分数:不同的子空间会产生不具有可比性的分数;</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>147</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>⑤分数的可解释性:分数通常不再具有语义上的意义;</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>148</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>⑥指数搜索空间:搜索空间无法进行系统性地扫描;</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>149</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>⑦Hubness:某些对象容易频繁地出现在其它对象的近邻列表中。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>150</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>目前,许多专门的方法只针对这些问题中的一个问题进行研究,留下很多开放性的问题值得我们继续分析讨论,在本章接下来的章节中将会对hubness这一现象进行深入研究。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>151</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>Hubness 现象</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>152</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>在机器学习领域,受维数灾难影响的方法和任务包括贝叶斯建模(Bishop,2006)、最近邻预测(Hastie et al,2009)及搜索(Korn et al,2001)等。维数灾难造成的影响之一是距离集中(Distance Concentration),这是说在高维数据中点对之间的距离渐渐趋向于相同。Hinneburg和Aggarwal等人已经对高维数据中的距离集中和无意义的最近邻作了深入的研究。</span><a href='../sentence_detail/185.htm' target='right'><span class='orange'>维数灾难造成的另一方面影响是hubness。</span></a><span class='green'>Hubness是一种某些对象容易频繁地出现在其它对象的k近邻列表中的现象。令表示一组数据点,其中为数据集D的元素。令dist表示在空间中的一个距离函数,其中如下定义:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>153</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>(3.4)</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>154</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>在此基础之上,定义,表示为在空间中,样本点x出现在其它样本点的k近邻列表中的次数,也记为k-occurrence或hubness score,仅仅根据数据点k-occurrence的大小无法确定hubness对实验结果有何种影响。数据点的bad k-occurrences表示为,是指样本点x作为数据集D中其它的样本点的k近邻次数,并且样本点x的标签和那些样本点的标签不匹配。数据点的good k-occurrences表示为,是指样本点x的标签与那些样本点的标签相匹配[2]。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>155</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>k-occurrences的分布与维数的关系</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>156</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>为了研究分布与数据集维数的关系进行了以下的实验分析。通过使用k-occurrences分布的标准第三矩(也称作偏度)来表征的非对称性[27],</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>157</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>(3.5)</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>158</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>其中和分别是的均值和标准差。</span><a href='../sentence_detail/186.htm' target='right'><span class='orange'>偏度常常用于概率学和统计论中,衡量实数域中随机变量分布的不对称性。</span></a><span class='green'></span><a href='../sentence_detail/187.htm' target='right'><span class='orange'>偏度的值有正负之分,偏度为负则表明绝大多数的值(包括中值在内)位于平均值的右侧;</span></a><span class='green'></span><a href='../sentence_detail/188.htm' target='right'><span class='orange'>偏度为正则表明绝大多数的值(不一定包括中值)位于平均值的左侧;</span></a><span class='green'></span><a href='../sentence_detail/189.htm' target='right'><span class='orange'>偏度为零则表明数值近似地均匀分布在均值的两侧,却不一定为对称分布。</span></a><span class='green'>偏度一般分为两种:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>159</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/190.htm' target='right'><span class='orange'>①左偏态或负偏态:数据的主体集中在右侧,</span></a><span class='green'>左侧会呈现出较长的尾部;</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>160</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/191.htm' target='right'><span class='orange'>②右偏态或正偏态:数据的主体集中在左侧,</span></a><span class='green'>右侧会呈现出较长的尾部。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>161</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>图 3.1 负偏态(左)和正偏态(右)</span><br><span class='green'>Figure 3.1 Negative skew(left) and positive skew(right)</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>162</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>若用三阶标准矩来表示随机变量X的偏度,那么偏度可被定义为:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>163</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>(3.6)</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>164</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>其中为三阶中心矩,为标准方差,E为期望算子。等式最终以三阶累积量和二阶累积量的1.5次方的比值来表示偏度。偏度也可用非中心矩来表示,公式如下所示:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>165</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>(3.7)</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>166</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>实验的数据集为从[0,1]均匀分布中随机抽取10000个维数为d的样本点,这些样本点之间彼此独立,采用以下三种距离度量方式来构建k近邻列表:欧几里德距离()、fractional 以及cosine[28]。图5(a-c)描述的是当时,的分布情况,其中数据集的维数d分别为:(a),(b),(c)。同样,图5(d-f)描述的是从正态分布中随机抽样获得的数据集的的分布情况。需要说明的一点是,只是一个经验值,当k取其它值时也可获得类似的结果。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>167</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>图 3.2在不同维数、不同距离度量方式情况下样本数为10000的数据集的N5分布图</span><br><span class='green'>Figure 3.2 Distribution of N5 for Euclidean, l0.5, and cosine distances on data sets with n = 10000 points and dimensionality (a, d) d = 3, (b, e) d = 20, and (c, f) d = 100.</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>168</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/192.htm' target='right'><span class='orange'>从图3.2的描述中可以看出,</span></a><span class='green'>当数据集的维数时,在三种度量方式下的分布近似于二项分布(图5(a,d))。这表明在低维数据空间中,随机取样的点的双向图中度的分布近似于Erdo ̋s-Re ́nyi(ER)随机图模型的度分布。然而随着数据集维数的增加,的分布将会逐渐偏离随机图模型的分布而且开始向右倾斜(图5(b,c)以及图5(e,f)其中度量方式为欧式距离和fractional )。通过上述实验可以观测到,在高维数据空间中的正偏态和hubs确实存在关联性,而且的偏度越大与之对应的数据集的hubness现象越强烈。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>169</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>为了进一步研究的偏度与数据集维数的相关性,</span><a href='../sentence_detail/193.htm' target='right'><span class='orange'>采用斯皮尔曼等级相关系数进行评测。</span></a><span class='green'>斯皮尔曼等级相关系数(Spearman correlation)是用于评估两个变量相关性的非参数指标[29],记作。对于样本数为n的数据集,为其对应的等级数据,那么相关系数的公式如下:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>170</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>(3.8)</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>171</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>由于在现实应用中变量之间的连结并没有显著作用,因此可以对进行如下简化[30]:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>172</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>(3.9)</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>173</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>其中,表示被评估的两个变量等级之间的差值,n为样本数。斯皮尔曼相关系数阐述了X(独立变量)与Y(依赖变量)的相关性。若变量X增加时,</span><a href='../sentence_detail/194.htm' target='right'><span class='orange'>变量Y也增加,那么斯皮尔曼相关系数的值为正数;若变量X增加时,变量Y却在减少,那么斯皮尔曼相关系数的值为负数;</span></a><span class='green'>若变量X和变量Y没有相关性,</span><a href='../sentence_detail/195.htm' target='right'><span class='orange'>那么斯皮尔曼相关系数则为零。</span></a><span class='green'>Milosˇ Radovanovic ́等人研究了50多个真实数据库的维数与其的斯皮尔曼相关系数(0.62),这一结果进一步说明数据集的维数和hubness现象存在着强烈的正相关性。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>174</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>Hubs的位置</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>175</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>为了进一步探究hubness这一现象,需要对hubs位置的分布进行深入分析。数据集仍然采用之前的均匀分布和正态分布随机取样数据。以样本数据分布的均值作为参考点,可以观测到样本点的k-occurrences值与其在数据空间中分布的相互关系。图6描述了在不同维数下(,,)每个数据样本x的值与其距离样本均值的相关性,其中。</span><a href='../sentence_detail/196.htm' target='right'><span class='orange'>从下图中可以看出,随着数据集维数的增加,</span></a><span class='green'>与该样本到数据集均值的距离出现了强烈的负相关性,这意味着越接近样本均值的点越有可能为hubs。</span><a href='../sentence_detail/197.htm' target='right'><span class='orange'>值得注意的是,只是一个经验值,</span></a><span class='green'>当k为其它值时也可获得相似的结果。该实验结果表明在高维数据空间中,当潜在的数据分布是单峰时,hubs会接近整体样本分布的均值;而当潜在的数据分布为多峰时(如多个单峰分布混合而成),hubs趋向于最近的单峰分布的均值。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>176</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>图 3.3在不同维数、不同距离度量方式情况下样本数为10000的数据集的N5(x)分布图</span><br><span class='green'>Figure 3.3 Scatter plots and Spearman correlation of N5(x) against the Euclidean distance of point x to the sample data-set mean for data sets with (a, d) d = 3, (b, e) d = 20, and (c, f) d = 100.</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>177</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>到目前为止,hubness现象对机器学习应用的影响还没有进行彻底的研究。在接下来的章节中,将会针对聚类分析这一领域对hubness进行深入的研究。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>178</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>Hub聚类算法分析</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>179</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>基于距离的聚类算法的主要目标是最小化同一个簇内对象之间的距离同时最大化簇间对象之间的距离。然而在高维数据空间中,无论是在efficiency还是effectiveness上,高维数据都对传统聚类算法造成了实质上的困难。因此,必须使用一种新的技术来对高维数据进行聚类分析。通常的想法是将原始的高维数据映射为一个较低维的流型结构[31],然后再进行聚类分析,该思想的代表算法是子空间聚类算法。在许多实际应用中,例如文本聚类和主题检测[32][33],为了进行有意义地聚类分析通常会将原始数据投影到某些较低维的子空间和流型结构(manifolds)中。</span><a href='../sentence_detail/198.htm' target='right'><span class='orange'>一般来说有两种类型的子空间聚类方法,</span></a><span class='green'></span><a href='../sentence_detail/199.htm' target='right'><span class='orange'>一种方法是试图找到一个真实的特征子空间,</span></a><span class='green'>另一种方法是在模拟过程中自动对特征进行加权处理以增加聚类效果。从理论上来说,在较低维的划分子空间中执行标准聚类算法看似是可行的[34]。然而,如果划分出的较低维子空间并不是真正意义上的低维数据,那么标准的聚类算法是无法处理的。此外,许多子空间聚类算法是基于密度的聚类或者是K-Means算法的扩展,而基于密度的聚类算法和K-Means聚类算法均不适用于高维数据聚类分析。由此可见,子空间聚类方法在高维数据空间中也存在诸多限制。因此,在接下来的章节中将会采用一种不同的方法,通过利用最近提出的k近邻图中出现的hubness现象进行聚类分析。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>180</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>在高维数据空间中,hubness现象将会对基于距离的聚类算法造成两方面影响。一方面,具有低k-occurrences的样本点很可能会增加簇内对象之间的距离,使得这些点远离数据集的其它点,可以将其视为离群点。目前,关于离群点在聚类分析方面的应用已经作了诸多的研究,通常离群点被发现之后会直接将其移除。另一方面,具有高k-occurrences的样本点,也就是hubs,很有可能会接近簇的中心。值得注意的是,一些聚类算法因为hubs的存在而使聚类性能变差,这是因为某些hubs会接近来自不同簇的样本点[2]。之前已经提到过,相比其它样本点而言,k-occurrences值越高的样本点越容易接近簇的均值,随之而来便产生了一个疑问:hubs会是当前簇的中值样本吗?Nenad Toma sev等人通过实验研究发现[35]:在低维数据空间中,hubs远离簇的中心甚至远离普通的点。</span><a href='../sentence_detail/200.htm' target='right'><span class='orange'>然而,随着数据集维数的增加,</span></a><span class='green'>簇的中心到hubs的最小距离会逐渐收敛于簇的中心到簇的中值样本的最小距离。这表明一些簇中值样本就是hubs。然而,簇的中心到hubs的最大距离却没有上述的相关性。同时观测到随着每一次的聚类迭代,簇的中心到hubs的最大聚类也逐渐减小,这就表明簇的中心越来越接近hubs。因此在高维数据中,hubs可以在很大程度上代表该簇中的元素。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>181</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>图 3.4 通过一个简单的例子说明在划分聚类迭代过程hubs原型与簇中心原型或簇中值原型的差别:红色虚线的圆圈代表簇的中心(C),黄色的点状圆圈代表簇的中值样本(M),绿色圆圈代表两个hubs(H1,H2),其中最近邻居数为3。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>182</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>Figure 3.4 An example of the difference between using hubs and centroids/medoids as cluster prototypes in partitional clustering iterations: The red dashed circle marks the centroid (C), yellow dotted circle the medoid (M), and green circles denote two elements of highest hubness (H1,H2), for neighborhood size 3.</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>183</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>既然hubs可以被视为一种度量局部中心性的方法,那么可以采取多种方式将hubs应用到各种聚类分析中。</span><a href='../sentence_detail/201.htm' target='right'><span class='orange'>在K-Means迭代过程中,</span></a><span class='green'>簇中心依赖当前簇中的所有元素,而hubs仅依赖它们的近邻元素,因此hubs携带着很多局部的中心性信息。Hubs主要可分为全局hubs和局部hubs。局部hubs是全局hubs在给定任一簇情况下的约束。因此,局部hubs的k-occurrences值是指在同一个簇中的某个样本点的k-occurrences的数量。同时,簇的中心和簇的中值样本容易趋向于k-occurrences值较大的样本点,也就是hubs,这意味着使用hubs作为簇原型可以加快算法的收敛速度。为了解释这一点我们设计了一个简单的模型如图3.4所示,图3.4通过2维数据空间模拟了高维数据空间中经常出现的hubness现象,该图阐释了以hubs作为簇原型不仅可以加快算法的收敛速度而且有助于发现更好的簇结构。Tomasev等人提出了基于hubness的K-Means扩展聚类算法[35]。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>184</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>在global K-hubs(GKH)算法中,hubs取代簇中心作为每次迭代过程中的簇原型。初步实验表明GKH方法容易过早地收敛到次优的簇结构。因此,</span><a href='../sentence_detail/202.htm' target='right'><span class='orange'>在GKH算法中引入了随机因子,</span></a><span class='green'>在每次迭代过程中hubs与其它样本点以某种概率被选为簇原型,该概率依赖样本点本身的值。这种算法被称为global hubness-proportional clustering(GHPC)。同样值得注意的是,相比基于密度的聚类分析,将hubs作为簇原型进行聚类分析的优点不仅仅是hubs能够很好地反映局部簇的中心性,而且hubs对数据集的规模不敏感。当数据集是由若干个密度差异较大的簇组成时,这一性质变得至关重要。向上或向下伸缩簇并不会改变其近邻结构,通常来说,也不会改变数据集的hubness特性。自然地,将hubs作为簇原型进行聚类分析并不一定能够对于任何数据集都可以得到最优的簇结构。有时,将簇中心作为簇原型反而可以得到不错的簇结构。</span><a href='../sentence_detail/203.htm' target='right'><span class='orange'>因此,需要对GHPC聚类算法进行改进和扩展:</span></a><span class='green'>在确定性迭代过程中使用簇中心作为簇原型;在随机性迭代过程中使用使用样本的随机概率作为簇原型。这种混合的方法叫做global hubness-proportional K-means(GHPKM)。由于GKH、GHPC和GHPKM聚类算法只能够发现超球形的簇[36],所以将核方法引入hubs聚类算法以便可以发现不同类型的簇结构。</span><a href='../sentence_detail/204.htm' target='right'><span class='orange'>下面将会详细介绍不同的hub聚类算法。</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>185</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>Deterministic方法</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>186</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>使用hubs进行聚类分析的一种简单方法是将hubs作为每次迭代过程中当前簇的簇原型,该算法一般称为K-hubs算法,其算法思想如下:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>187</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>Algorithms 1. K-hubs</span><br><span class='green'>initializeClusterCenters();</span><br><span class='green'>Cluster[] clusters = formClusters();</span><br><span class='green'>repeat</span><br><span class='green'>​	for all Cluster c  clusters do</span><br><span class='green'>​			DataPoint h = findClusterHub(c);</span><br><span class='green'>​			setClusterCenter(c, h);</span><br><span class='green'>​	end for</span><br><span class='green'>clusters = formClusters();</span><br><span class='green'>until noReassignments</span><br><span class='green'>return clusters</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>188</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/205.htm' target='right'><span class='orange'>尽管K-hubs聚类算法可以得到很好的聚类效果,</span></a><span class='green'></span><a href='../sentence_detail/206.htm' target='right'><span class='orange'>但是它对初始簇原型十分敏感,</span></a><span class='green'>而且容易获得次优的聚类结构。为了增加找到全局最优解的概率,下面将介绍随机变量的K-hubs聚类分析算法。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>189</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>Probabilistic方法</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>190</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>尽管拥有最高k-occurrences值的样本点可以最大可能地代表当前簇的信息,但是簇中其它样本点也有可能包含该簇的重要信息。因此,在K-hubs算法上对簇原型的选择加入了一定的随机性,通过使用模拟退火方法实现了一个平方hubness-proportional的随机方法[37],将温度因子引入到K-hubs算法中,那么它的初始簇原型就是完全随机的,该方法称为hubness-proportional clustering(HPC)聚类算法,其算法思想如下:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>191</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>Algorithm 2. HPC.</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>192</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>initializeClusterCenters();</span><br><span class='green'>Cluster[] clusters = formClusters();</span><br><span class='green'>float t = t0; initialize temperature</span><br><span class='green'>repeat</span><br><span class='green'>​	float  = getProbFromSchedule(t);</span><br><span class='green'>​	for all Cluster c  clusters do</span><br><span class='green'>​		if randomFloat(0,1) ^  then</span><br><span class='green'>​			DataPoint h = findClusterHub(c);</span><br><span class='green'>​			setClusterCenter(c, h);</span><br><span class='green'>​		else</span><br><span class='green'>​			for all DataPoint x  c do</span><br><span class='green'>​				setChoosingProbability(x, );</span><br><span class='green'>​			end for</span><br><span class='green'>​			normalizeProbabilities();</span><br><span class='green'>​			DataPoint h = chooseHubProbabilistically(c);</span><br><span class='green'>​			setClusterCenter(c, h);</span><br><span class='green'>​		end if</span><br><span class='green'>​	end for</span><br><span class='green'>​	clusters = formClusters();</span><br><span class='green'>​	t = updateTemperature(t);</span><br><span class='green'>until noReassignments</span><br><span class='green'>return clusters</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>193</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>在高维数据空间中,Hubness-proportional 聚类分析算法的可行性在于k-occurrences偏度的统计分布。在k-occurrences偏度的统计分布中,绝大多数的点拥有较低的值,这通常意味会它们会被GHPC算法忽略掉,因为它们被视为是十分差的簇原型的候选者而且它们被选择的概率也非常低。关于选择样本点的方法,GHPC算法采用了一个相当繁琐的温度因子方案,当然其它的随机方案也是可行的,甚至会产生更好的聚类效果。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>194</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>Hybird方法</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>195</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>K-hubs聚类算法和GHPC聚类算法都没有关注数据或对象的表现形式(representation),它们只在意样本间的距离矩阵。然而,</span><a href='../sentence_detail/207.htm' target='right'><span class='orange'>如果数据的表现形式是已知的,</span></a><span class='green'>那么便可以利用簇中心的相关性质进行聚类,同时使用样本点的k-occurrences指导聚类搜索,最终会形成一个基于簇中心的聚类结构。该算法被称为hubness-proportional K-means(HPKM)聚类算法,它与GHPC聚类算法的唯一不同之处在于迭代过程中的确定阶段使用的是K-Means更新簇原型而非K-hubs。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>196</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>Algorithm 3. HPKM.</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>197</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>initializeClusterCenters();</span><br><span class='green'>Cluster[] clusters = formClusters();</span><br><span class='green'>float t = t0; initialize temperature</span><br><span class='green'>repeat</span><br><span class='green'>​	float  = getProbFromSchedule(t);</span><br><span class='green'>​	for all Cluster c  clusters do</span><br><span class='green'>​		if randomFloat(0,1) ^  then</span><br><span class='green'>​			DataPoint h = findClusterCentroid(c);</span><br><span class='green'>​			setClusterCenter(c, h);</span><br><span class='green'>​		else</span><br><span class='green'>​			for all DataPoint x  c do</span><br><span class='green'>​				setChoosingProbability(x, );</span><br><span class='green'>​			end for</span><br><span class='green'>​			normalizeProbabilities();</span><br><span class='green'>​			DataPoint h = chooseHubProbabilistically(c);</span><br><span class='green'>​			setClusterCenter(c, h);</span><br><span class='green'>​		end if</span><br><span class='green'>​	end for</span><br><span class='green'>​	clusters = formClusters();</span><br><span class='green'>​	t = updateTemperature(t);</span><br><span class='green'>until noReassignments</span><br><span class='green'>return clusters</span><br><span class='green'>Kernel GHPKM方法</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>198</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>K-hubs、GHPC和GHPKM算法的主要缺陷是它们只能发现发现超球面的簇。</span><a href='../sentence_detail/208.htm' target='right'><span class='orange'>然而在现实生活中簇的形状是任意的,</span></a><span class='green'>所以需要寻找一种新的技术来解决此问题。Kernel K-Means算法[38]是K-Means算法的一个扩展,同样K-hubs、GHPC和GHPKM算法也是K-means的扩展,因此可以将这些算法与kernel方法结合起来进行聚类分析。令表示数据集的簇,其中,使用非线性函数,那么kernel GHPKM聚类算法的目标函数如下:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>199</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>(3.10)</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>200</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/209.htm' target='right'><span class='orange'>其中,为每个样本点所对应的权重,</span></a><span class='green'>为每个簇的原型,这是kernel K-means算法和kernel GHPKM算法的第一个不同之处:</span><a href='../sentence_detail/210.htm' target='right'><span class='orange'>在kernel K-means算法中,</span></a><span class='green'>并不一定是簇的中心,也可以是hubs或者其它的样本点。然而,</span><a href='../sentence_detail/211.htm' target='right'><span class='orange'>随着迭代次数的不断增加,这个不同之处将会越来越小甚至消失。</span></a><span class='green'>这一现象是由模拟退火算法的降温方法所引起的,</span><a href='../sentence_detail/212.htm' target='right'><span class='orange'>随着迭代次数的增加随机性发生的概率将越来越小,</span></a><span class='green'>最终演变成确定性的迭代过程。因此,Kernel GHPKM算法可以使用与kernel K-Means算法相同的最小化函数,同时也可以通过随机选择初始值来避免局部最优值的问题。令为簇c的中心,在映射函数下,簇中心可通过下面的公式(3.11)计算得出。有时为了区分簇原型和簇hub原型,我们通常用和分别标记。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>201</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>(3.11)</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>202</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>从下面的等式中可以看出,簇中心的最优化目标是通过最小化加权映射距离的平方和获得的。虽然使用hubs作为簇原型并不会最小化平方距离和,但是它却会带来其它的益处。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>203</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>(3.12)</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>204</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/213.htm' target='right'><span class='orange'>Algorithm 4. Kernel GHPKM.</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>205</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>initializeClusterCenters();</span><br><span class='green'>float t = t0; initialize temperature</span><br><span class='green'>repeat</span><br><span class='green'>​	float  = getProbFromSchedule(t);</span><br><span class='green'>​	for all Point x  dataset do</span><br><span class='green'>​		closestCluster = NULL;</span><br><span class='green'>​		minimalDistance = MAX VALUE;</span><br><span class='green'>​		for all Cluster c  clusters do</span><br><span class='green'>​			if getClusterCenter(c) NOT NULL then</span><br><span class='green'>​				distance = getDistanceToHub(c);</span><br><span class='green'>​				if distance  minimalDistance then</span><br><span class='green'>​					minimalDistance = distance;</span><br><span class='green'>​					closestCluster = c;</span><br><span class='green'>​				end if</span><br><span class='green'>​			else</span><br><span class='green'>​				distance = getDistanceToCentroid(c);</span><br><span class='green'>​				if distance ≤ minimalDistance then</span><br><span class='green'>​					minimalDistance = distance;</span><br><span class='green'>​					closestCluster = c;</span><br><span class='green'>​				end if</span><br><span class='green'>​			end if</span><br><span class='green'>​		end for</span><br><span class='green'>​		assignPointToFutureCluster(x, closestCluster)</span><br><span class='green'>​	end for</span><br><span class='green'>​	updateClusterAssignments();</span><br><span class='green'>​	for all Cluster c ∈ clusters do</span><br><span class='green'>​		if randomFloat(0,1) ^  then</span><br><span class='green'>​			setClusterCenter(c, NULL);</span><br><span class='green'>​		else</span><br><span class='green'>​			for all DataPoint x  c do</span><br><span class='green'>​				setChoosingProbability(x, );</span><br><span class='green'>​			end for</span><br><span class='green'>​			normalizeProbabilities();</span><br><span class='green'>​			DataPoint h = chooseHubProbabilistically(c);</span><br><span class='green'>​			setClusterCenter(c, h);</span><br><span class='green'>​		end if</span><br><span class='green'>​	end for</span><br><span class='green'>​	t = updateTemperature(t);</span><br><span class='green'>​	calculateErrorFunction();</span><br><span class='green'>until convergenceCriterion</span><br><span class='green'>clusters = formClusters();</span><br><span class='green'>return clusters</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>206</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>本节详细地介绍四种hub聚类算法,</span><a href='../sentence_detail/214.htm' target='right'><span class='orange'>并归纳出了各自的优缺点和适用范围。</span></a><span class='green'>从上述的阐述中可以发现,以特定方式将hubs最为簇原型不仅可以获得更好的簇结构同时也可以加快聚类分析的收敛速度。从本质上说,如果在聚类分析算法的迭代过程中当前的簇是由多个紧凑的部分组成,那么簇的中心和簇的中值样本并不一定能够代表有意义的原型。理想情况下,在搜索最佳的划分和最优簇结构时,我们希望在每一次的迭代过程中都能将独立的子部分分给不同的簇。然而,以簇中心和簇中值样本作为簇原型很可能会减小迭代之间的差异,增加迭代次数甚至得到次优的簇结构。此外,多组分簇(multi-component clusters)的簇中心和簇的中值样本并不能与局部组分簇中心对应。因此,使用hubs作为搜索原型可以克服在高维数据空间中进行聚类分析的相关问题。虽然hub聚类算法相比经典聚类算法中在高维数据空间中表现出了显著优势,然而它却没有关注高维数据空间中的冗余和噪声数据,</span><a href='../sentence_detail/215.htm' target='right'><span class='orange'>因此并未获得更优的聚类效果。</span></a><span class='green'></span><a href='../sentence_detail/216.htm' target='right'><span class='orange'>下面的章节将会针对这一问题进行深入的分析研究,</span></a><span class='green'>并提出可行的更改方案。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>207</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>Hub聚类算法的改进</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>208</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>基于距离的聚类算法的主要目标是最小化同一个簇内对象之间的距离同时最大化簇间对象之间的距离。在高维数据空间中,样本的k-occurrences偏度将会对上述两个对象造成影响。一方面,具有低k-occurrences的样本点很可能会增加簇内对象之间的距离,这些样本点远离数据集的其它点,可以将其视为离群点。目前,关于离群点在聚类分析方面的应用已经作了诸多的研究,通常离群点被发现之后会直接将其移除。另一方面,具有高k-occurrences的样本点,也就是hubs,很有可能会接近簇的中心。另外,数据集的hubness度依赖于数据集的本征维数而非嵌入维数(embedding dimensionality)[39][40]。本征维数(Intrinsic dimensionality)是指表示数据集所有点对之间的距离所需特征的最小数量[41]。通常,hubness与本征维数相关而与距离或相似度的度量方式无关。通常,较低的k-occurrences值表明该样本点远离数据样本中的其它点,并且很有可能是一个离群点。然而,在高维数据空间中,由于数据本身的分布情况使得较低的k-occurrences样本点变得很普遍,这些样本点将会增加簇内样本之间的距离。同样值得注意的是,一些聚类算法因为hubs的存在而使聚类性能变差。这是因为某些hubs会接近来自不同簇的点[35]。之前已经提到过,相比其它点而言,k-occurrences值越高的样本点越容易接近簇的中心,Nenad Toma sev等人通过实验研究发现在高维数据空间中,hubs可以在很大程度上代表当前簇中的元素,从而获得更好的聚类结构和更快的聚类收敛速度。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>209</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>虽然hub聚类算法利用了hubs在高维数据空间中的特性,并获得了较为不错的聚类效果,然而它却没有考虑高维数据空间中的冗余和噪声数据,</span><a href='../sentence_detail/217.htm' target='right'><span class='orange'>因此并未获得更优的聚类效果。</span></a><span class='green'>接下来的章节将会通过分析的偏度与本征维数的相互关系,探究降维技术是否能够缓解的偏度等问题,</span><a href='../sentence_detail/218.htm' target='right'><span class='orange'>从而进一步对hub聚类算法进行改进,</span></a><span class='green'>其中降维技术使用的是主成分分析方法。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>210</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>主成分分析</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>211</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>在多变量的统计分析中,</span><a href='../sentence_detail/219.htm' target='right'><span class='orange'>主成分分析(Principal components analysis,PCA)常常用于分析和简化数据集[42]。</span></a><span class='green'>主成分分析通过保留对方差贡献最大的样本特征,从而降低数据集的维数。Pearson于1901年发明了主成分分析[43],常常用于数据分析以及模型建立。主成分分析的主要思想是将协方差矩阵进行特征分解,从而获得数据的主要成分(特征向量)及其对应的权重(特征值)。</span><a href='../sentence_detail/220.htm' target='right'><span class='orange'>使用主成分分析算法对数据集进行降维处理后可以极大地提升无监督特征学习的速度。</span></a><span class='green'>以下是主成分分析算法的具体思想:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>212</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>①使用n行d列的矩阵X表示原始数据;</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>213</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>②将矩阵X的每一列进行零均值化,即减去这一行的均值;</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>214</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>③求解协方差矩阵;</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>215</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/221.htm' target='right'><span class='orange'>④求解协方差矩阵的特征值及其特征向量;</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>216</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>⑤令特征向量按照其对应的特征值降序排序,取前k列组成新的矩阵P;</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>217</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>⑥即为降维后新的数据。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>218</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>主成分分析基于最大方差矩阵理论,</span><a href='../sentence_detail/222.htm' target='right'><span class='orange'>通过协方差矩阵的特征向量选择k维理想特征,</span></a><span class='green'>也就是说,</span><a href='../sentence_detail/223.htm' target='right'><span class='orange'>在减少数据集维数的同时保留数据集中对方差贡献最大的特征。</span></a><span class='green'></span><a href='../sentence_detail/224.htm' target='right'><span class='red'>这是通过保留低阶主成分,忽略高阶主成分做到的。</span></a><span class='green'></span><a href='../sentence_detail/225.htm' target='right'><span class='red'>这样低阶成分往往能够保留住数据的最重要方面。</span></a><span class='green'></span><a href='../sentence_detail/226.htm' target='right'><span class='red'>主成分分析主要是通过对协方差矩阵进行特征分解,以得出数据的主成分(即特征向量)与它们的权值(即特征值)。</span></a><span class='green'></span><a href='../sentence_detail/227.htm' target='right'><span class='red'>这可以理解为对原数据中的方差做出解释:哪一个方向上的数据值对方差的影响最大?</span></a><span class='green'></span><a href='../sentence_detail/228.htm' target='right'><span class='orange'>换而言之,PCA提供了一种降低数据维度的有效办法;</span></a><span class='green'>如果分析者在原数据中除掉最小的特征值所对应的成分,那么所得的低维度数据必定是最优化的(也即,这样降低维度必定是失去讯息最少的方法)。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>219</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/229.htm' target='right'><span class='orange'>如何选择k值,即保留多少个主成分?</span></a><span class='green'>对于k值的选择,通常以k值所保留的方差百分比作为参考依据。一般来说,</span><a href='../sentence_detail/230.htm' target='right'><span class='orange'>当时保留了百分之百的方差,也就是说原先数据的所有变化均被保留了下来;相反,</span></a><span class='green'>当时只保留了百分之零的方差。</span><a href='../sentence_detail/231.htm' target='right'><span class='orange'>通常而言,令表示协方差矩阵的特征值(从大到小排列),特征值对应的特征向量为,</span></a><span class='green'></span><a href='../sentence_detail/232.htm' target='right'><span class='orange'>如果选择k个主成分那么保留的方差百分比可表示为:</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>220</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>(3.13)</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>221</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>通常而言,通过选择最小的k值使得保留方差的范围位于90~98%之间,在不同的应用领域中这个范围可自行调整。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>222</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>基于偏度的降维方法</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>223</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>关于数据降维的方法有多种,本文采用的是主成分分析法。</span><a href='../sentence_detail/233.htm' target='right'><span class='red'>主成分分析经常用于减少数据集的维数,同时保持数据集中的对方差贡献最大的特征。</span></a><span class='green'>当没有任何假设信息的信号模型时,主成分分析在降维的同时并不能保证信息的不丢失,其中信息是由香农熵来衡量的。然而,香农熵却无法作为数据有效降维时的衡量标准,因此本文采用了的偏度这一指标。下文中将会探讨在使用降维技术PCA的情况下的偏度和本征维数的相互作用。此研究的主要目的在于探讨降维是否能够缓解的偏度这一问题。“因为观察到的偏度与本征维数强烈正相关,本征维数对到数据集的均值或到最接近簇的均值有着积极影响,这意味着在较高(本征)维数的数据集中,hubs变得越来越接近数据集的中心或者最接近的簇的中心”。</span><a href='../sentence_detail/234.htm' target='right'><span class='orange'>实验过程中采用的距离度量方法是闵可夫斯基距离(Minkowski distance)[44],它是一种非常常见的衡量样本点之间距离的方法,</span></a><span class='green'>假设数值点P和Q坐标如下:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>224</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/235.htm' target='right'><span class='orange'>那么,闵可夫斯基距离定义为:</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>225</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>(3.14)</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>226</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>该距离最常用的p值是2和1,</span><a href='../sentence_detail/236.htm' target='right'><span class='orange'>前者是欧几里得距离(Euclidean distance)[45],</span></a><span class='green'></span><a href='../sentence_detail/237.htm' target='right'><span class='orange'>后者是曼哈顿距离(Manhattan distance)[46]。</span></a><span class='green'>可夫斯基距离虽然比较直观明了,但是它并没有考虑数据的统计分布,因此具有某些局限性。例如,若x方向上的幅值比</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>227</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>y方向的幅值要大得多,那么闵可夫斯基距离将会在很大程度上扩大x方向上的作用。因此,在计算对象之间的距离之前,根据数据的分布情况可能需要进行z-transform处理,即减去该维度上的均值,并除以其标准差:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>228</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>(3.15)</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>229</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>其中,是当前维度上的均值,是当前维度上的标准差。由此可见,z-transform是基于数据在各维度上不相关的假设,并利用数据的统计分布特性进行不同的距离度量的。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>230</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>为了探究在使用降维技术的情况下的偏度和本征维数的相互作用,本文使用了来自加州大学尔湾分校(UCI)机器学习库[47]的数据集进行观测的分布。在表3.1中包含了以下信息:数据集的名称(第1列);数据集的样本数(n,第2列);数据样本的特征维数(d,第3列);数据集簇的个数(cls,第4列)。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>231</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>表 3.1来自UCI机器学习库的真实数据集</span><br><span class='green'>Table 3.1 Real data sets from UCI Machine Learning Repository</span><br><span class='green'>数据集</span><br><span class='green'>样本数</span><br><span class='green'>维数</span><br><span class='green'>簇的个数</span><br><span class='green'>arrhythmia</span><br><span class='green'>452</span><br><span class='green'>279</span><br><span class='green'>10</span><br><span class='green'>Ionosphere</span><br><span class='green'>351</span><br><span class='green'>34</span><br><span class='green'>2</span><br><span class='green'>mfeat-factors</span><br><span class='green'>2000</span><br><span class='green'>216</span><br><span class='green'>10</span><br><span class='green'>mfeat-fou</span><br><span class='green'>2000</span><br><span class='green'>76</span><br><span class='green'>10</span><br><span class='green'>musk</span><br><span class='green'>476</span><br><span class='green'>166</span><br><span class='green'>2</span><br><span class='green'>spectrometer</span><br><span class='green'>531</span><br><span class='green'>100</span><br><span class='green'>10</span><br><span class='green'>sonar</span><br><span class='green'>208</span><br><span class='green'>60</span><br><span class='green'>2</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>232</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>图3.5描述了针对若干个真实数据集(musk,sonar,mfeat-fou等)通过降维方法获得的维数占原有数据集维数的百分比与之间的相互关系。数据之间距离的度量方法为Minkowski距离,其中p的取值分别为:2(欧几里得距离)。从左往右观察,对于大部分数据集而言利用PCA降维算法,保持相对恒定直到降维后留下特征的百分比较小时才会陡然下降。因此,当达到数据集的本征维数时若继续减小维数则会导致有价值的信息丢失。</span><a href='../sentence_detail/238.htm' target='right'><span class='orange'>针对PCA方法对数据进行降维时,</span></a><span class='green'>若降维后本征维数未发生明显变化,那么降维并不会对hubness这一现象有显著影响。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>233</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>图 3.5 N10的偏度与降维维数的关系</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>234</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>Figure 3.5 Skewness of N10 in relation to the percentage of the original number of features maintained by dimensionality reduction.</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>235</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>PCA-Hub聚类算法</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>236</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>上节通过实验研究发现,数据集的偏度与数据集的维数存在强烈正相关,更确切地说,是与数据集的本征维数存在强烈正相关,而数据集的本征维数表示的是数据集所有点对之间的距离所需特征的最小数量[41]。因此,</span><a href='../sentence_detail/239.htm' target='right'><span class='orange'>本文提出了PCA-Hub聚类算法,</span></a><span class='green'>此算法以数据集的偏度作为降维衡量标准,通过不断减小数据集的原始维数来逐渐逼近数据集的本征维数,这样不仅不会损失数据集的“原始”信息,而且还能消除其中的冗余和噪声数据,有利于更快的发现更优的簇结构。下面是PCA-Hub聚类算法的具体步骤:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>237</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>①数据预处理</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>238</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>实验之前首先要观察数据并获知数据的特性,并且应该针对具体的数据采取合适的预处理技术。本章采用的数据预处理技术为一种常见的数据归一化方法------逐样本均值消减(也被称为移除直流分量,局部均值消减,消减归一化),即对于每个样本点减去数据统计分布的平均值[53]。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>239</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>②构造KNN邻域矩阵</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>240</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>基于距离的聚类算法需要考虑不同的距离度量方法对于聚类性能的影响,不同类型的数据集应该采用各自适合的距离度量方法。在确定合适的距离度量方式之后,需要选定合适的近邻数k用于构建KNN邻域矩阵。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>241</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>③计算逆近邻偏度</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>242</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>通过KNN邻域矩阵可获得每个样本点的逆近邻数,通过偏度可以衡量样本逆近邻数的非对称性,并以此分析数据集的hubness情况。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>243</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>④PCA降维</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>244</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>因为逆近邻的偏度与数据集的本征维数强烈正相关,因此将偏度作为主成分分析的降维指标,当偏度小于某一设定的阈值时便可认为数据集已损失了较多的本征维数,即剩下的维数为理想的k个主成分。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>245</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>⑤聚类分析</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>246</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>由于hub可以代表局部中心性,</span><a href='../sentence_detail/240.htm' target='right'><span class='orange'>所以可以用hub聚类算法对降维后的数据进行聚类分析。</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>247</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>图 3.6 PCA-Hub聚类算法流程图</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>248</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>Figure 3.6 PCA-Hub clustering algorithm flow chart.</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>249</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>实验结果及其分析</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>250</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>本小节将会从三个方面分别探讨PCA-Hub聚类算法的聚类性能,具体情况如下:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>251</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>①PCA-Hub聚类算法的轮廓系数</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>252</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>实验数据来源于加州大学尔湾分校(UCI)机器学习库。表3.2中第5列为真实数据集的偏度值,其中10代表k近邻数。从表中数据可以看出,对于大多数数据集的的分布发生了倾斜。虽然k的值是固定的,但是使用其它的k值也可得到类似的结果。采用轮廓系数作为聚类结果的评测指标[48]。本文方法与KMEANS[35]、GHPKM[35]、Ker-KM[36]和 Ker-KM[36]方法进行了比较,其中PH-KM为本文的聚类方法。实验结果如表3.2所示,下表中加粗的数据表示当前数据集的最优值。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>253</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>表 3.2 UCI库中数据集的聚类质量(轮廓系数)</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>254</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>Table 3.2 Clustering quality expressed as silhouette index on data sets from the UCI repository.</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>255</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>数据集</span><br><span class='green'>样本数</span><br><span class='green'>维数</span><br><span class='green'>簇的个数</span><br><span class='green'>距离度量</span><br><span class='green'>KME</span><br><span class='green'>ANS</span><br><span class='green'>[9]</span><br><span class='green'>GHPKM</span><br><span class='green'>[9]</span><br><span class='green'>Ker-KM [4]</span><br><span class='green'>Ker-GHPKM</span><br><span class='green'>[4]</span><br><span class='green'>PH-KM</span><br><span class='green'>Ionosphere</span><br><span class='green'>351</span><br><span class='green'>34</span><br><span class='green'>2</span><br><span class='green'>1.72</span><br><span class='green'>l2</span><br><span class='green'>0.28</span><br><span class='green'>0.28</span><br><span class='green'>0.28</span><br><span class='green'>0.25</span><br><span class='green'>0.41</span><br><span class='green'>mfeat-factors</span><br><span class='green'>2000</span><br><span class='green'>216</span><br><span class='green'>10</span><br><span class='green'>0.83</span><br><span class='green'>l2</span><br><span class='green'>0.18</span><br><span class='green'>0.20</span><br><span class='green'>0.17</span><br><span class='green'>0.18</span><br><span class='green'>0.24</span><br><span class='green'>musk</span><br><span class='green'>2000</span><br><span class='green'>166</span><br><span class='green'>2</span><br><span class='green'>1.33</span><br><span class='green'>l2</span><br><span class='green'>0.28</span><br><span class='green'>0.28</span><br><span class='green'>0.29</span><br><span class='green'>0.29</span><br><span class='green'>0.31</span><br><span class='green'>parkinsons</span><br><span class='green'>195</span><br><span class='green'>22</span><br><span class='green'>2</span><br><span class='green'>0.73</span><br><span class='green'>l2</span><br><span class='green'>0.42</span><br><span class='green'>0.44</span><br><span class='green'>0.64</span><br><span class='green'>0.21</span><br><span class='green'>0.88</span><br><span class='green'>sonar</span><br><span class='green'>208</span><br><span class='green'>60</span><br><span class='green'>2</span><br><span class='green'>1.35</span><br><span class='green'>l2</span><br><span class='green'>0.20</span><br><span class='green'>0.21</span><br><span class='green'>0.26</span><br><span class='green'>0.17</span><br><span class='green'>0.22</span><br><span class='green'>wpbc</span><br><span class='green'>198</span><br><span class='green'>33</span><br><span class='green'>2</span><br><span class='green'>0.86</span><br><span class='green'>l2</span><br><span class='green'>0.16</span><br><span class='green'>0.16</span><br><span class='green'>0.32</span><br><span class='green'>0.22</span><br><span class='green'>0.31</span><br><span class='green'>AVG-UCI</span><br><span class='green'>0.25</span><br><span class='green'>0.26</span><br><span class='green'>0.33</span><br><span class='green'>0.22</span><br><span class='green'>0.39</span><br><span class='green'>图 3.6 UCI库中数据集的聚类质量(轮廓系数)</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>256</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>Figure 3.6 Clustering quality expressed as silhouette index on data sets from the UCI repository.</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>257</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>对于每一个数据集而言,取KMEANS、GHPKM、Ker-KM以及Ker-GHPKM聚类算法中轮廓系数的最大值作为经典聚类算法的最优值,然后同本文的PH-KM聚类算法进行比较。</span><a href='../sentence_detail/241.htm' target='right'><span class='orange'>实验结果表明,相比之前的聚类算法,</span></a><span class='green'>本文提出的PH-KM聚类算法在轮廓系数上平均提高了15%。</span><a href='../sentence_detail/242.htm' target='right'><span class='orange'>从表3.2的实验结果可以看出,</span></a><span class='green'>经典的KMEANS聚类算法更适用于低维数据聚类;在数据集缺乏hubness特性的情况下,GHPKM、Ker-GHPKM等hub聚类算法表现不佳,其性能接近于KMEANS算法;然而当数据集呈现出较高的hubness特性时,GHPKM、Ker-GHPKM等hub 聚类算法的表现要优于KMEANS算法。同时,本文提出的 PCA-Hub聚类算法无论数据集是否呈现出较高的hubness特性,均可以取得不错的聚类效果,相比之前的聚类算法适用范围更广,聚类性能更佳。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>258</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>②PCA-Hub聚类算法对近邻数k的敏感程度</span><br><span class='green'>图 3.7 PCA-Hub聚类算法对近邻数k的敏感程度</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>259</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>Figure 3.7 The sensitivity of PCA-Hub Clustering Algorithm to k nearest Neighbors.</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>260</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>由于PCA-Hub聚类算法是基于K-Means聚类算法在高维数据空间的扩展方法,因此有必要研究其对于近邻数的敏感程度。实验所用的数据集和距离度量方法仍然保持不变,PCA-Hub聚类在每个数据集上均重复聚类50次,近邻数k的取值范围从5到25。图3.7为实验结果示意图,从图中可以看出当数据集的维数较低且的偏度也不高时,PCA-Hub聚类算法对近邻数k这一参数的选择表现出了明显的依赖性,</span><a href='../sentence_detail/243.htm' target='right'><span class='orange'>聚类算法的性能在很大程度上取决于近邻数的取值;同时,</span></a><span class='green'>图3.7表明当数据集本身的维数较高时或者的偏度不低时,PCA-Hub聚类算法在使用不同的近邻数k时表现出了相似的聚类性能,因此近邻数的选择对于PCA-Hub聚类算法的聚类结果影响并不强烈。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>261</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>③PCA-Hub聚类算法聚类结果的一致性</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>262</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>为了研究PCA-Hub聚类算法结果的稳定性或一致性,本文进行了如下的实验研究:采用之前的UCI数据库,设置参数近邻数k为最优值,聚类算法的重复次数为50次,并记录每一次的聚类结果。从图3.8中可以看出,在实验环境和聚类分析算法参数一致的情况下,PCA-Hub聚类算法在开始的一小段重复次数时发生了些许的波动,但随着聚类算法重复次数的增加,聚类结果渐渐趋于稳定,并最后收敛于某一个恒定的值,这一现象表明PCA-Hub聚类算法的聚类性能,尤其是聚类重复次数比较高的情况下,在很大程度上具有一致性。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>263</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>图 3.8 PCA-Hub聚类算法对近邻数k的敏感程度</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>264</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>Figure 3.8 The sensitivity of PCA-Hub Clustering Algorithm to k nearest Neighbors.</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>265</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>本章小结</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>266</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>本章节首先对高维数据空间中的维数灾难做了简要的分析介绍,并归纳出了维数灾难在机器学习中的影响。针对这种现象引入了hubness这一较新的概念,并对hubness做了十分详尽的描述:首先给出了hubness现象相关的定义,并分析了hubs在数据集中的位置;然后根据hubs的中心性特征介绍了hub聚类算法,并归纳出了其优缺点。通过研究发现虽然hub聚类算法可以在高维数据空间中进行聚类分析,但是却忽略了高维数据中的冗余和噪声数据,从而导致聚类效果不佳。随后,通过的偏度来表征数据集的hubness特性,并以的偏度与本征维数强烈正相关为理论基础,通过构建数据集的KNN邻域矩阵,以偏度的变化率作为降维依据选出理想的k个主成分,之后再对降维后的数据集进行聚类分析。最后通过多次实验分别从聚类结果的好坏(轮廓系数)、对近邻数k的敏感程度和聚类结果的一致性三方面进行了深入分析,</span><a href='../sentence_detail/244.htm' target='right'><span class='orange'>实验结果表明,在聚类结果方面,</span></a><span class='green'>无论数据集是否呈现出较高的hubness特性,本章提出的PCA-Hub聚类算法均可以取得不错的聚类效果,相比之前的聚类算法,轮廓系数平均提高了15%;在对近邻数k的敏感程度方面,PCA-Hub聚类算法在数据集本身的维数较高或者的偏度不低时,对近邻数k的选择表现不强烈;在聚类结果的一致性方面,PCA-Hub聚类算法在实验环境和聚类算法参数一致的情况下,聚类结果在很大程度上具有一致性。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>267</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>错误!未找到引用源。错误!未找到引用源。PCA-Hub 聚类算法</span><br><span class='green'>3 PCA-Hub聚类算法</span><br><span class='green'>Quick PCA-Hub聚类算法</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>268</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>在第三章PCA-Hub聚类算法分析中,以偏度的变化率作为降维依据,利用主成分分析降维方法对数据集进行降维的同时尽可能地保留了数据集的本征维数,从而提高了聚类算法的性能。虽然PCA-Hub聚类算法可以解决高维数据中的冗余和噪声特征,并且降维后的数据集也可以加快聚类分析的速度和获得不错的簇结构,但是在获取主成分分析方法的k值时,尤其对高维数据而言,该阶段的计算代价过于昂贵。因此需要找到一种可以快速获得主成分分析方法中理想k值的算法,本章将会从快速搜索k个主成分和快速搜索最近邻居两方面介绍加快PCA-Hub聚类算法的速度,其中图4.1为Quick PCA-Hub聚类算法的流程图。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>269</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>图 4.1 Quick PCA-Hub聚类算法流程图</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>270</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>Figure 4.1 Quick PCA-Hub clustering algorithm flow chart.</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>271</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>快速搜索k个主成分</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>272</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>Quick PCA-Hubness聚类算法的整体流程如下所示:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>273</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>①数据预处理</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>274</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>实验之前首先要观察数据并获知数据的特性,并且应该针对具体的数据采取合适的预处理技术。本章采用的数据预处理技术为一种常见的数据归一化方法------逐样本均值消减(也被称为移除直流分量,局部均值消减,消减归一化),即对于每个样本点减去数据统计分布的平均值[53]。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>275</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>②构造KNN邻域矩阵</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>276</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>基于距离的聚类算法需要考虑不同的距离度量方法对于聚类性能的影响,不同类型的数据集应该采用各自适合的距离度量方法。在确定合适的距离度量方式之后,需要选定合适的近邻数k用于构建KNN邻域矩阵。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>277</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>③计算逆近邻偏度</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>278</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>通过KNN邻域矩阵可获得每个样本点的逆近邻数,通过偏度可以衡量样本逆近邻数的非对称性,并以此分析数据集的hubness情况。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>279</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>④Quick PCA降维</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>280</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>因为逆近邻的偏度与数据集的本征维数强烈正相关,因此将偏度作为主成分分析的降维指标,当偏度小于某一设定的阈值时便可认为数据集已损失了较多的本征维数,即剩下的维数为理想的k个主成分。为了加快此过程的搜寻速度,本章作了以下优化:首先将数据集的维数进行p等分并求出其对应的偏度,当该处偏度小于设定的阈值时停止运算;然后,针对此区间将这p等分的样本继续进行q等分,计算每一处的偏度直至该处偏度小于设定的阈值时停止运算,至此便可快速找到理想的k个主成分。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>281</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>⑤聚类分析</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>282</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>由于hub可以代表局部中心性,</span><a href='../sentence_detail/245.htm' target='right'><span class='orange'>所以可以用hub聚类算法对降维后的数据进行聚类分析。</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>283</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>0. 算法思想</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>284</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>Quick PCA-Hub聚类算法首先对数据集进行预处理,将数据的每一维进行归一化;其次,构建KNN邻域矩阵,计算每个点的逆近邻数。然后,用PCA进行降维,在降维的过程中通过偏度的变化率来控制降维的程度,以防损失过多重要的有价值信息。最后,在获取降维数据后利用hub聚类算法进行聚类分析。下面是Q</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>285</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>uick PCA-Hub的聚类算法思想:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>286</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>Algorithm. Qucik PCA-Hub.</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>287</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>float[][] knn=getKNN();</span><br><span class='green'>float[] Nk =getSkewness(knn);</span><br><span class='green'>float[][] eigenvectors=pca();</span><br><span class='green'>Dataset new=getKpca();</span><br><span class='green'>initializeClusterCenters();</span><br><span class='green'>Cluster[] clusters = formClusters();</span><br><span class='green'>float t = t0; initialize temperature</span><br><span class='green'>repeat</span><br><span class='green'>​	float  = getProbFromSchedule(t);</span><br><span class='green'>​	for all Cluster c  clusters do</span><br><span class='green'>​		if randomFloat(0,1) ^  then</span><br><span class='green'>​			DataPoint h = findClusterHub(c);</span><br><span class='green'>​			setClusterCenter(c, h);</span><br><span class='green'>​		else</span><br><span class='green'>​			for all DataPoint x  c do</span><br><span class='green'>​				setChoosingProbability(x, );</span><br><span class='green'>​			end for</span><br><span class='green'>​			normalizeProbabilities();</span><br><span class='green'>​			DataPoint h = chooseHubProbabilistically(c);</span><br><span class='green'>​			setClusterCenter(c, h);</span><br><span class='green'>​		end if</span><br><span class='green'>​	end for</span><br><span class='green'>​	clusters = formClusters();</span><br><span class='green'>​	t = updateTemperature(t);</span><br><span class='green'>until noReassignments</span><br><span class='green'>return clusters</span><br><span class='green'>0. 实验结果及其分析</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>288</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>实验数据来源于加州大学尔湾分校(UCI)数据库,表4.1中第5列为真实数据集的偏度值,其中10代表近邻数k。虽然k值是固定的,但是使用其它的k值也可得到类似的结果。表4.1中第12列(迭代数)为搜索理想的k个主成分所需的次数,第13列(减少的维数)为数据集降维后所损失的维数。本章的Quick PCA-Hub算法分别与KMEANS、GHPKM、Ker-KM和Ker-GHPKM聚类算法进行比较,其中轮廓系数为聚类结果的评测指标,迭代数和减少的维数为搜索k个主成分的速度指标。实验结果如表4.1所示,下表中加粗的数据表示当前数据集的最优值。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>289</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>表 4.1 UCI库中数据集的聚类质量(轮廓系数)</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>290</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>Table 4.1 Clustering quality expressed as silhouette index on data sets from the UCI repository.</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>291</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>数</span><br><span class='green'>据</span><br><span class='green'>集</span><br><span class='green'>样本数</span><br><span class='green'>维数</span><br><span class='green'>簇的个数</span><br><span class='green'>K-M</span><br><span class='green'>E</span><br><span class='green'>ANS</span><br><span class='green'>[9]</span><br><span class='green'>GH</span><br><span class='green'>PKM</span><br><span class='green'>[9]</span><br><span class='green'>Ker-KM  [4]</span><br><span class='green'>Ker-GHP</span><br><span class='green'>KM  [4]</span><br><span class='green'>PH-KM</span><br><span class='green'>Q PH-KM</span><br><span class='green'>迭代数</span><br><span class='green'>减少的维数</span><br><span class='green'>Ionosphere</span><br><span class='green'>351</span><br><span class='green'>34</span><br><span class='green'>2</span><br><span class='green'>1.72</span><br><span class='green'>0.28</span><br><span class='green'>0.28</span><br><span class='green'>0.28</span><br><span class='green'>0.25</span><br><span class='green'>0.41</span><br><span class='green'>0.40</span><br><span class='green'>8</span><br><span class='green'>11</span><br><span class='green'>mfeat-factors</span><br><span class='green'>2000</span><br><span class='green'>216</span><br><span class='green'>10</span><br><span class='green'>0.83</span><br><span class='green'>0.18</span><br><span class='green'>0.20</span><br><span class='green'>0.17</span><br><span class='green'>0.18</span><br><span class='green'>0.24</span><br><span class='green'>0.15</span><br><span class='green'>7</span><br><span class='green'>88</span><br><span class='green'>musk</span><br><span class='green'>2000</span><br><span class='green'>166</span><br><span class='green'>2</span><br><span class='green'>1.33</span><br><span class='green'>0.28</span><br><span class='green'>0.28</span><br><span class='green'>0.29</span><br><span class='green'>0.29</span><br><span class='green'>0.31</span><br><span class='green'>0.28</span><br><span class='green'>10</span><br><span class='green'>107</span><br><span class='green'>parkinsons</span><br><span class='green'>195</span><br><span class='green'>22</span><br><span class='green'>2</span><br><span class='green'>0.73</span><br><span class='green'>0.42</span><br><span class='green'>0.44</span><br><span class='green'>0.64</span><br><span class='green'>0.21</span><br><span class='green'>0.88</span><br><span class='green'>0.61</span><br><span class='green'>7</span><br><span class='green'>7</span><br><span class='green'>sonar</span><br><span class='green'>208</span><br><span class='green'>60</span><br><span class='green'>2</span><br><span class='green'>1.35</span><br><span class='green'>0.20</span><br><span class='green'>0.21</span><br><span class='green'>0.26</span><br><span class='green'>0.17</span><br><span class='green'>0.22</span><br><span class='green'>0.20</span><br><span class='green'>9</span><br><span class='green'>20</span><br><span class='green'>wpbc</span><br><span class='green'>198</span><br><span class='green'>33</span><br><span class='green'>2</span><br><span class='green'>0.86</span><br><span class='green'>0.16</span><br><span class='green'>0.16</span><br><span class='green'>0.32</span><br><span class='green'>0.22</span><br><span class='green'>0.31</span><br><span class='green'>0.51</span><br><span class='green'>8</span><br><span class='green'>11</span><br><span class='green'>AVG-UCI</span><br><span class='green'>0.25</span><br><span class='green'>0.26</span><br><span class='green'>0.33</span><br><span class='green'>0.22</span><br><span class='green'>0.39</span><br><span class='green'>0.36</span><br><span class='green'>8</span><br><span class='green'>41</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>292</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>下面主要从聚类结果的好坏和搜寻k个主成分的速度两个方面阐述Quick PCA-Hub聚类算法的聚类性能:</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>293</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>①聚类结果的好坏------轮廓系数</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>294</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>对于每一个数据集而言,取KMEANS、GHPKM、Ker-KM和Ker-GHPKM聚类算法中轮廓系数最大值作为经典聚类算法的最优值,然后同本章的Quick PCA-Hub聚类算法进行比较。</span><a href='../sentence_detail/246.htm' target='right'><span class='orange'>实验结果表明,相比之前的聚类算法,</span></a><span class='green'>本章提出的Quick PCA-Hub聚类算法在轮廓系数上提高了8%。</span><a href='../sentence_detail/247.htm' target='right'><span class='orange'>从表4.1和图4.2的实验结果可以看出,</span></a><span class='green'>经典的KMEANS聚类算法更适用于低维数据聚类;在数据集缺乏hubness特性的情况下,GHPKM、Ker-GHPKM等hub聚类算法表现不佳,其性能接近于KMEANS算法;然而当数据集呈现出较高的hubness特性时,GHPKM、Ker-GHPKM等hub聚类算法的表现要优于KMEANS算法。同时,本文提出的Quick PCA-Hub聚类算法无论数据集是否呈现出较高的hubness特性,均可以取得不错的聚类效果,相比之前的聚类算法适用范围更广,聚类性能更佳。从UCI平均数据集的轮廓系数可以观测到,本章的Quick PCA-Hub聚类算法要优于之前的聚类算法,但逊于第三章的PCA-Hub聚类算法。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>295</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>图 4.2 PCA-Hub聚类算法对近邻数k的敏感程度</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>296</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>Figure 4.2 The sensitivity of PCA-Hub Clustering Algorithm to k nearest Neighbors.</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>297</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>②搜寻k个主成分的速度</span><br><span class='green'>图 4.3 PCA-Hub聚类算法对近邻数k的敏感程度</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>298</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>Figure 4.3 The sensitivity of PCA-Hub Clustering Algorithm to k nearest Neighbors.</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>299</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>从表4.1中第12列的迭代数和第13列的减少的维数可以看出,本章的Quick PCA-Hub在高维数据空间中搜索理想的k个主成分时表现出了巨大的优势,然而当数据集的维数不高时Quick PCA-Hub聚类算法的加速效果并不明显。同时可以看出,当数据集有较高的hubness特性时,Quick PCA-Hub聚类算法不仅加速搜寻k个主成分的速度,</span><a href='../sentence_detail/248.htm' target='right'><span class='orange'>而且可以获得更优的聚类结果;</span></a><span class='green'>然而当数据集有较低的hubness特性时,Quick PCA-Hub聚类算法的聚类优化效果则不明显。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>300</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>快速搜索最近邻居</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>301</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/249.htm' target='right'><span class='orange'>最邻近搜索(Nearest Neighbor Search, NNS),</span></a><span class='green'>亦称为“最近点搜索”(Closest point search),是指在一个尺度空间里搜索最近点的最优化问题[49]。可以对问题进行如下描述:在尺度空间M中,存在一个点集S以及一个目标点,在点集S中找到离目标点q最近的点。最近邻搜索有多种解决方法,这些方法的性能取决于它们求解的时间复杂度以及搜索的空间复杂度。朴素最近邻搜索需要遍历整个点集,</span><a href='../sentence_detail/250.htm' target='right'><span class='orange'>计算目标点与其它点之间的距离,</span></a><span class='green'>并记录当前的最近点。朴素最近邻搜索较为初级,适用于较小规模的点集,但是对于较大尺度的点集和较高的空间维数并不适用。在最邻近搜索的几个变化中,最著名的是KNN(K-nearest neighbor algorithm)[50]和近似最邻近查找(-approximate nearest neighbor search)[51]。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>302</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>Hub聚类算法需要计算KNN的完全图。由于未利用任何空间数据结构或近似计算的技术,朴素KNN图的计算复杂度在处理大型数据集时将会变得十分昂贵,但是可通过快速近似方法在合理的时间内构建一个十分精准的近似图。关于快速近似方法可以使Chen等人提出的通用方法[52]或者使用基于locality-sensitive hashing的特定度量近似方法[53]。不同的近似KNN搜索方法的性能取决于解决特定问题时数据集的数据特征和在特定环境下k近邻的本征难度[54]。近年来,人们开始关注在模糊和不确定数据集中随机逆k近邻查询的计算复杂度[55]。Tomasˇev等人针对一些hub聚类算法作了实验研究,实验结果表明即使在线性时间内构建KNN图也不会明显降低算法性能[2]。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>303</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>本章小结</span><br><span class='green'>重庆大学硕士学位论文</span><br><span class='green'>重庆大学硕士学位论文</span><br><span class='green'>4 Quick PCA-Hub聚类算法</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>304</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>本章分别从快速搜索k个主成分和快速搜索最近邻居两方面增加PCA-Hub算法的聚类分析速度:首先,Quick PCA-Hub算法分别与经典聚类算法和PCA-Hub算法进行了对比分析。通过实验证明,Quick PCA-Hub算法相比经典聚类算法可以取得不错的聚类结果,而且当数据集的维数较高时,Quick PCA-Hub算法在搜索理想的k个主成分时表现出了巨大的优势。其次,从理论上探讨了快速搜索最近邻居的方法。虽然在大型高维数据数据空间中朴素KNN图的构建十分耗时,但是可以通过近似KNN搜索方法在合理的时间内构建一个十分精确的近似图。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>305</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>总结与展望</span><br><span class='green'>总结</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>306</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/251.htm' target='right'><span class='orange'>聚类分析被视为数据挖掘的一个十分重要的研究领域,</span></a><span class='green'>常常被用于分析现实生活的大量未知数据,从而发现其中重要的有价值信息和知识。</span><a href='../sentence_detail/252.htm' target='right'><span class='orange'>随着科技的发展,现实生活中未知数据的数量越来越多,</span></a><span class='green'></span><a href='../sentence_detail/253.htm' target='right'><span class='orange'>聚类分析在实际应用中的地位也越来越重要。</span></a><span class='green'>但同时数据集的尺度也越来越大,数据的维数也越来越高,这些问题将不断地向传统的聚类分析算法提出挑战。基于不同的理论以及聚类模型,</span><a href='../sentence_detail/254.htm' target='right'><span class='orange'>研究人员提出了多种聚类分析算法。</span></a><span class='green'>针对高维数据空间中维数灾难这一问题,hub聚类算法利用高维数据空间的特征,可以解决传统聚类算法无法在高维数据空间中聚类分析的问题。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>307</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>Hubness是最近几年才提出的一个较为新颖的概念,</span><a href='../sentence_detail/255.htm' target='right'><span class='orange'>常被应用于有监督的机器学习中,</span></a><span class='green'>例如分类和回归问题,而在无监督的机器学习中则研究不多。本文针对hubness这一概念,对将其应用到聚类分析中作了详尽的分析研究,</span><a href='../sentence_detail/256.htm' target='right'><span class='orange'>所得出的主要研究和结论如下:</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>308</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>①详尽地概述了数据挖掘和聚类分析,分别从数据挖掘的定义、作用、发现过程以及应用等方面,</span><a href='../sentence_detail/257.htm' target='right'><span class='orange'>系统地对其进行了全面的介绍。</span></a><span class='green'>接着阐述了聚类分析的定义及其主流的聚类模型,重点分析比较了常用的聚类分析算法,并归纳出了主流的聚类分析算法的适用范围及优缺点,同时介绍了聚类分析的评价标准和评估指标,并分析了聚类分析算法的现状和发展趋势。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>309</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>②详尽地概述了维数灾难这一现象,并归纳出了在搜索高维空间数据时由于维数灾难可能导致的问题,本文深入地研究了其中的一个问题------hubness。针对hubness这一现象,首先对其进行了详细地描述并给出了形式化的定义,然后对其本质特征进行了仔细分析,包括表征的非对称性、hubs的在统计分布中的位置等等。根据这些特征进一步研究了hubs在聚类分析中的作用,并介绍了相关的hub聚类算法,同时归纳总结出了其各自的优缺点及适用范围。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>310</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>③针对hub聚类算法未处理高维数据空间中的冗余和噪声特征,因此本文提出了PCA-Hub聚类算法。PCA-Hub聚类算法是以的偏度与本征维数强烈正相关为理论基础,通过构建数据集的KNN邻域矩阵,以偏度的变化率作为降维依据选出理想的k个主成分,之后再对降维后的数据集进行聚类分析。实验分别从聚类结果的好坏(轮廓系数)、对近邻数k的敏感程度和聚类结果的一致性三方面进行了分析,实验结果表明,无论数据集是否呈现出较高的hubness特性,PCA-Hub聚类算法均可以取得不错的聚类效果,相比之前的聚类算法,轮廓系数平均提高了15%;当数据集本身的维数较高时或者的偏度不低时,PCA-Hub聚类算法在使用不同近邻数k时表现出了相似的聚类性能,因此近邻数的选择对于PCA-Hub聚类算法的聚类结果影响并不强烈;</span><a href='../sentence_detail/258.htm' target='right'><span class='orange'>在实验环境和聚类算法参数一致的情况下,</span></a><span class='green'>PCA-Hub聚类算法的结果在很大程度上具有一致性。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>311</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>④PCA-Hub聚类算法虽然可以很好地解决高维数据空间中的冗余和噪声数据,</span><a href='../sentence_detail/259.htm' target='right'><span class='orange'>然而随着数据集尺度和数据集维数的不断增加,</span></a><span class='green'>PCA-Hub聚类算法的耗时将会变得越来越严重甚至是不可接受。因此,本文分别从快速搜索k个主成分和快速搜索最近邻居两方面增加PCA-Hub算法的聚类分析速度。通过实验证明,Quick PCA-Hub算法相比经典聚类算法可以取得不错的聚类结果,而且当数据集的维数较高时,Quick PCA-Hub算法在搜索理想的k个主成分时表现出了巨大的优势。其次,从理论上探讨了快速搜索最近邻居的方法。虽然在大型高维数据数据空间中朴素KNN图的构建十分耗时,但是可以通过近似KNN搜索方法在合理的时间内构建一个十分精确的近似图,而且近似的KNN图并不会显著影响聚类结果。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>312</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>展望</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>313</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>本人在高维数据空间中的聚类分析领域进行了一些研究并获得了一定的成果,但是由于本人的科学研究水平以及研究时间等因素的限制,论文中存在一些不足之处尚待改进以及尚未完成的研究,在未来的研究工作中还需要对以下的几个方面进行深入研究。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>314</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>①PCA-Hub聚类算法在解决高维数据空间中的冗余和噪声数据时,</span><a href='../sentence_detail/260.htm' target='right'><span class='orange'>需要预先设定偏度下降的阈值。</span></a><span class='green'>如果阈值设置的不合理,</span><a href='../sentence_detail/261.htm' target='right'><span class='orange'>那么聚类分析的结果可能就不理想。</span></a><span class='green'>在今后的工作中,希望可以设置一个自适应的阈值来控制偏度下降的程度,从而降低PCA-Hub聚类算法对参数设置的敏感性。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>315</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>②进一步探索不同的近似KNN搜索方法对PCA-Hub聚类算法的影响,以便可以找到一种合适的方法来在合理的时间内构建KNN图。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>316</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>5 总结与展望</span><br><span class='green'>致谢</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>317</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>随着毕业时间的临近,</span><a href='../sentence_detail/262.htm' target='right'><span class='red'>三年的研究生学习生涯也即将结束。</span></a><span class='green'>回顾这几年的时光,有老师、同学以及朋友的亲切陪伴,一路走来,虽有辛劳,却也收获了成功的满足,值此,</span><a href='../sentence_detail/263.htm' target='right'><span class='orange'>向三年内关心帮助过我的老师、同学以及朋友们表示由衷的感谢。</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>318</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/264.htm' target='right'><span class='orange'>首先要向我的导师葛亮老师致以深深的谢意,</span></a><span class='green'>整个研究生阶段从入学进校到论文撰写准备毕业,</span><a href='../sentence_detail/265.htm' target='right'><span class='orange'>全程都贯穿着葛亮老师的热情关怀和悉心指导。</span></a><span class='green'>三年来,葛老师多次加班加点帮我修改论文,多次为我的研究方案提出建议,无不使我获益匪浅,深受启发,并帮助我逐渐学会了如何去面对问题,思考问题最终圆满的解决问题。葛老师细致严谨、实事求是的作风态度都深深的影响了我,使我受益良多,</span><a href='../sentence_detail/266.htm' target='right'><span class='red'>再次向葛老师致以我最诚恳的谢意。</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>319</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>三年研究生阶段中,课题组朱庆生老师、舒立春老师、张志劲老师、胡琴老师在学习生活中给予了我极大的关心和鼓励,此外实验室的廖瑞金老师,王有元老师,李剑老师,杨丽君老师,杨庆老师,袁涛老师等也在各方面给予了我诸多帮助,在此向他们致以由衷的谢意!</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>320</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>然后还要真诚感谢已毕业的吴尧师兄,从进校的时候手把手指导我完成试验,其后耐心指导我进行科技论文写作,以及毕业后仍不忘跟我交流学习、科研以及工作心得,吴尧师兄给予我的帮助实在难以用言语表达;此外孙晓峰,刘健两位师兄也对我科研、生活给予了无私帮助,在此一并感谢。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>321</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>另外我取得的科研成绩也离不开同窗许可的大力协助以及杨洪椿,石璧,钟睿,李洋洋等师弟们的配合。而在科研期间,与赵洪彬,陈勇,白洋,王慕宾,范才进,罗异等各位同学的交流也给予我诸多启发。此外实验室的各位同学也给予我诸多指导和帮助,</span><a href='../sentence_detail/267.htm' target='right'><span class='orange'>在此也向他们表达我最真诚的谢意。</span></a><span class='green'>另外好友余瑜,彭珊,陈祥等也在我低谷时给予我诸多鼓励和帮助,在此一并表示感谢。</span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>322</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'></span><a href='../sentence_detail/268.htm' target='right'><span class='orange'>谨以此文献给辛勤养育我的父母,</span></a><span class='green'></span><a href='../sentence_detail/269.htm' target='right'><span class='orange'>感谢他们二十多年来的无私付出以及对我求学的支持,并为我创造了良好的学习,生活条件,</span></a><span class='green'></span><a href='../sentence_detail/270.htm' target='right'><span class='orange'>在此,向他们致以我最崇高的敬意!</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>323</div></td><td>&nbsp;&nbsp;</td></tr></table><span style='margin-left:25px'></span><span class='green'>最后,</span><a href='../sentence_detail/271.htm' target='right'><span class='orange'>衷心感谢在百忙之中抽出时间来评阅我论文和答辩的各位专家、教授!</span></a><span class='green'></span></p></div></p>
    		<p style="margin:2px"><div><p><table border='0' width='100%' cellspacing='0' cellpadding='0'><tr><td align='left' width='50'><div class='shubu'>324</div></td><td>&nbsp;&nbsp;</td></tr></table><span class='green'>郎江涛</span><br><span class='green'>二零一七年四月于重庆</span><br><span class='green'>致    谢</span><br><span class='green'>参考文献</span><br><span class='green'>[1] John,N.,Megatrends:Ten New Directions Transforming Our Lives. NY:Futura,1984:p. 28.</span><br><span class='green'>[2] Milosˇ Radovanovic ́,Alexandros Nanopoulos,MirjanaIvanovic ́. Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data[J],Journal of Machine Learning Research 11 (2010) 2487-2531. 2010</span><br><span class='green'>[3] John,N.,Megatrends:Ten New Directions Transforming Our Lives. NY:Futura,1984:p. 28</span><br><span class='green'>[4] Data Mining Curriculum. ACM SIGKDD. 2006-04-30.</span><br><span class='green'>[5] 潘有能,XML 挖掘:聚类、分类与信息提取. 杭州:浙江大学出版社,2012</span><br><span class='green'>[6] Lloyd, S. (1982). "Least squares quantization in PCM". IEEE Transactions on Information Theory. 28 (2): 129–137</span><br><span class='green'>[7] Ester, Martin; Kriegel, Hans-Peter; Sander, Jörg; Xu, Xiaowei (1996). "A density-based algorithm for discovering clusters in large spatial databases with noise". In Simoudis, Evangelos; Han, Jiawei; Fayyad, Usama M. Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD-96). AAAI Press. pp. 226–231.</span><br><span class='green'>[8] Ankerst, Mihael; Breunig, Markus M.; Kriegel, Hans-Peter; Sander, Jörg (1999). "OPTICS: Ordering Points To Identify the Clustering Structure". ACM SIGMOD international conference on Management of data. ACM Press. pp. 49–60.</span><br><span class='green'>[9] Roy, S.; Bhattacharyya, D. K. (2005). "An Approach to find Embedded Clusters Using Density Based Techniques". LNCS Vol.3816.</span><br><span class='green'>[10] Cheng, Yizong (August 1995). "Mean Shift, Mode Seeking, and Clustering". IEEE Transactions on Pattern Analysis and Machine Intelligence. IEEE. 17 (8): 790–799</span><br><span class='green'>[11] Xu, X.; Yan, Z.; Xu, S. (2015). "Estimating wind speed probability distribution by diffusion-based kernel density method". Electric Power Systems Research. 121: 28–37.</span><br><span class='green'>[12] Sculley, D. (2010). Web-scale k-means clustering. Proc. 19th WWW.</span><br><span class='green'>[13] Huang, Z. (1998). "Extensions to the k-means algorithm for clustering large data sets with categorical values". Data Mining and Knowledge Discovery. 2: 283–304.</span><br><span class='green'>[14] R. Ng and J. Han. "Efficient and effective clustering method for spatial data mining". In: Proceedings of the 20th VLDB Conference, pages 144-155, Santiago, Chile, 1994.</span><br><span class='green'>[15] Tian Zhang, Raghu Ramakrishnan, Miron Livny. "An Efficient Data Clustering Method for Very Large Databases." In: Proc. Int'l Conf. on Management of Data, ACM SIGMOD, pp. 103–114.</span><br><span class='green'>[16] McCallum, A.; Nigam, K.; and Ungar L.H. (2000) "Efficient Clustering of High Dimensional Data Sets with Application to Reference Matching", Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining, 169-178</span><br><span class='green'>[17] Can, F.; Ozkarahan, E. A. (1990). "Concepts and effectiveness of the cover-coefficient-based clustering methodology for text databases". ACM Transactions on Database Systems. 15 (4): 483–517.</span><br><span class='green'>[18] Manning, Christopher D.; Raghavan, Prabhakar; Schütze, Hinrich. Introduction to Information Retrieval. Cambridge University Press. ISBN 978-0-521-86571-5.</span><br><span class='green'>[19] Estivill-Castro, Vladimir (20 June 2002). "Why so many clustering algorithms — A Position Paper". ACM SIGKDD Explorations Newsletter. 4 (1): 65–75.</span><br><span class='green'>[20] Manning, Christopher D.; Raghavan, Prabhakar; Schütze, Hinrich. Introduction to Information Retrieval. Cambridge University Press. ISBN 978-0-521-86571-5</span><br><span class='green'>[21] Färber, Ines; Günnemann, Stephan; Kriegel, Hans-Peter; Kröger, Peer; Müller, Emmanuel; Schubert, Erich; Seidl, Thomas; Zimek, Arthur (2010). "On Using Class-Labels in Evaluation of Clusterings" (PDF). In Fern, Xiaoli Z.; Davidson, Ian; Dy, Jennifer. MultiClust: Discovering, Summarizing, and Using Multiple Clusterings. ACM SIGKDD</span><br><span class='green'>[22] Rand, W. M. (1971). "Objective criteria for the evaluation of clustering methods". Journal of the American Statistical Association. American Statistical Association. 66 (336): 846–850</span><br><span class='green'>[23] Powers, David M W (2011). "Evaluation: From Precision, Recall and F-Measure to ROC, Informedness, Markedness & Correlation" (PDF). Journal of Machine Learning Technologies. 2 (1): 37–63.</span><br><span class='green'>[24] Oommen, T. .; Misra, D. .; Twarakavi, N. K. C.; Prakash, A. .; Sahoo, B. .; Bandopadhyay, S. . An Objective Analysis of Support Vector Machine Based Classification for Remote Sensing. Mathematical Geosciences. 2008, 40 (4): 409. doi:10.1007/s11004-008-9156-6.</span><br><span class='green'>[25] Hughes, G.F., 1968. "On the mean accuracy of statistical pattern recognizers", IEEE Transactions on Information Theory, IT-14:55-63.</span><br><span class='green'>[26] Not to be confused with the unrelated, but similarly named, Hughes effect in electromagnetism (named after Declan C. Hughes) which refers to an asymmetry in the hysteresis curves of laminated cores made of certain magnetic materials, such as permalloy or mu-metal, in alternating magnetic fields.</span><br><span class='green'>[27] Groeneveld, RA; Meeden, G. Measuring Skewness and Kurtosis. The Statistician. 1984, 33 (4): 391–399</span><br><span class='green'>[28] Charu C. Aggarwal, Alexander Hinneburg, and Daniel A. Keim. On the surprising behavior of distance metrics in high dimensional spaces. In Proceedings of the 8th International Conference on Database Theory (ICDT), volume 1973 of Lecture Notes in Computer Science, pages 420– 434. Springer, 2001</span><br><span class='green'>[29] Maritz. J.S. (1981) Distribution-Free Statistical Methods, Chapman & Hall. ISBN 0-412-15940-6. (page 217)</span><br><span class='green'>[30] Myers, Jerome L.; Well, Arnold D., Research Design and Statistical Analysis 2nd, Lawrence Erlbaum: 508, 2003, ISBN 0-8058-4037-0</span><br><span class='green'>[31] Kriegel HP, Kr¨oger P, Zimek A (2009) Clustering high-dimensional data: A survey on subspaceclustering, pattern-based clustering, and correlation clustering. ACM Transactions on KnowledgeDiscovery from Data 3(1):1:1–1:58</span><br><span class='green'>[32] Jing L, Ng M, Xu J, Huang J (2005) Subspace clustering of text documents with feature weightingk-means algorithm. In: Ho T, Cheung D, Liu H (eds) Advances in Knowledge Discovery andData Mining, Lecture Notes in Computer Science, vol 3518, Springer Berlin Heidelberg, pp802–812</span><br><span class='green'>[33] Li T, Ma S, Ogihara M (2004) Document clustering via adaptive subspace iteration. In: Proceedingsof the 27th Annual International ACM SIGIR Conference on Research and Developmentin Information Retrieval, ACM, New York, NY, USA, SIGIR ’04, pp 218–225, DOI</span><br><span class='green'>[34] Jing L, Ng M, Huang J (2007) An entropy weighting k-means algorithm for subspace clusteringof high-dimensional sparse data. Knowledge and Data Engineering, IEEE Transactions on19(8):1026–1041</span><br><span class='green'>[35] Nenad Toma sev,Milo s Radovanovi c,DunjaMladeni c,andMirjana Ivanovi c. The Role of Hubness in Clustering High-Dimensional Data[J],IEEETRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 26, NO. 3,2014</span><br><span class='green'>[36] Amina M,Syed Farook K. A Novel Approach forClustering High-Dimensional Data using Kernel Hubness[J]. InternationalConfenrence on Advances in Computing and Communication. 2015.</span><br><span class='green'>[37] D. Corne, M. Dorigo, and F. Glover, New Ideas in Optimization. McGraw-Hill, 1999.</span><br><span class='green'>[38] Grigorios F. Tzortzis and Aristidis C. Likas, The Global Kernel-Means Algorithm forClustering in Feature Space IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 20, NO. 7, JULY 2009</span><br><span class='green'>[39] D. Donoho and C. Grimes, "Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data" Proc Natl Acad Sci U S A. 2003 May 13; 100(10): 5591–5596</span><br><span class='green'>[40] Rosman G., Bronstein M. M., Bronstein A. M. and Kimmel R., Nonlinear Dimensionality Reduction by Topologically Constrained Isometric Embedding, International Journal of Computer Vision, Volume 89, Number 1, 56–68, 2010</span><br><span class='green'>[41] Bennett, R. (June 1965). "Representation and analysis of signals—Part XXI: The intrinsic dimensionality of signal collections". Rep. 163 (PDF). Baltimore, MD: The Johns Hopkins University.</span><br><span class='green'>[42] Jolliffe I.T. Principal Component Analysis, Series: Springer Series in Statistics, 2nd ed., Springer, NY, 2002, XXIX, 487 p. 28 illus. ISBN 978-0-387-95442-4</span><br><span class='green'>[43] Pearson, K. On Lines and Planes of Closest Fit to Systems of Points in Space (PDF). Philosophical Magazine. 1901, 2 (6): 559–572</span><br><span class='green'>[44] Rolewicz, Stefan (1987), Functional analysis and control theory: Linear systems, Mathematics and its Applications (East European Series), 29 (Translated from the Polish by Ewa Bednarczuk ed.), Dordrecht; Warsaw: D. Reidel Publishing Co.; PWN—Polish Scientific Publishers, pp. xvi+524, ISBN 90-277-2186-6, MR 920371, OCLC 13064804</span><br><span class='green'>[45] Deza, Elena; Deza, Michel Marie (2009). Encyclopedia of Distances. Springer. p. 94.</span><br><span class='green'>[46] For most large underdetermined systems of linear equations the minimal 1-norm solution is also the sparsest solution; See Donoho, David L (2006). "For most large underdetermined systems of linear equations the minimal 1-norm solution is also the sparsest solution". Communications on pure and applied mathematics. 59: 797–829. doi:10.1002/cpa.20132</span><br><span class='green'>[47] Lichman, M.  UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science. 2013</span><br><span class='green'>[48] Peter J. Rousseeuw. Silhouettes: a Graphical Aid to the Interpretation and Validation of Cluster Analysis[J]. Computational and Applied Mathematics. 20: 53–65. 1987.</span><br><span class='green'>[49] Roussopoulos, N.; Kelley, S.; Vincent, F. D. R. (1995). "Nearest neighbor queries". Proceedings of the 1995 ACM SIGMOD international conference on Management of data – SIGMOD '95. p. 71. doi:10.1145/223784.223794. ISBN 0897917316.</span><br><span class='green'>[50] Altman, N. S. (1992). "An introduction to kernel and nearest-neighbor nonparametric regression". The American Statistician. 46 (3): 175–185. doi:10.1080/00031305.1992.10475879.</span><br><span class='green'>[51] Ma, Zongmin. Artificial Intelligence for Maximizing Content Based Image Retrieval. IGI Global. p. 135. ISBN 9781605661759.</span><br><span class='green'>[52] Chen J, ren Fang H, Saad Y (2009) Fast approximate kNN graph construction for high dimensional data via recursive Lanczos bisection. Journal of Machine Learning Research 10:1989–2012</span><br><span class='green'>[53] Satuluri V, Parthasarathy S (2012) Bayesian locality sensitive hashing for fast similarity search.Proc VLDB Endow 5(5):430–441</span><br><span class='green'>[54] He J, Kumar S, Chang SF (2012) On the difficulty of nearest neighbor search. In: International Conference on Machine Learning (ICML), icml.cc / Omnipress</span><br><span class='green'>[55] Zhang P, Cheng R, Mamoulis N, Renz M, Zufle A, Tang Y, Emrich T (2013) Voronoi-based nearest neighbor search for multi-dimensional uncertain databases. In: Data Engineering (ICDE), 2013 IEEE 29th International Conference on, pp 158–169, DOI 10.1109/ICDE.2013.6544822</span><br><span class='green'>[56] Pyle, D., 1999. Data Preparation for Data Mining. Morgan Kaufmann Publishers, Los Altos, California.</span><br><span class='green'>重庆大学硕士学位论文</span><br><span class='green'>参考文献</span><br><span class='green'>附  录</span><br><span class='green'>A.作者在攻读硕士学位期间撰写的论文目录</span><br><span class='green'>[1] 胡建林,蓝彬桓,许 可,蒋兴良,石 璧,杨洪椿,风机叶片运用疏水性涂层防覆冰的风洞试验研究,高电压技术 (EI期刊,已录用待刊出,稿件编号:20160025)</span><br><span class='green'>[2]</span><br><span class='green'>B.作者在攻读硕士学位期间参与的科研项目</span><br><span class='green'>[1]</span></p></div></p>
    </div>

<div class="zhengwencenter">
<p>
检测报告由<a href="http://www.paperfree.cn" target="_blank">PaperFree</a>文献相似度检测系统生成
</p>
</div>
<div style="margin-bottom:100px"></div>
<!--增加的代码------------------开始-->

<style type="text/css">
  div[align=right] *{
	cursor:pointer;
	-webkit-user-select: none;
    -moz-user-select: none;
  	-ms-user-select: none;
    -o-user-select: none;
    user-select: none;
 }
 
 div[align=right] input[type=button]{
	cursor:pointer;
	background: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAACA0lEQVQ4T6WSTWgTURDH//M2xRSlfmASU0S9iD3UD4xxAyoaRBTbbA+CHqS9CF48KwgKHtpLpRWP3kShBw9Cto1eBL0ou6kFv1IRoaKINBsoVASp5u04u1rZJumh9F3e7jzmN///zBBWeSia/7ln98a40X5NQR2ReBcDH+V+gboeSj6anG1V6z/AK5iHofCAQGlJ/EnMM3LvIKK1kjgH9gcSdrnUCAkBn47t27CuI14B8WYwXX1qO7fPAvoGoC5ZuQtMGCWwXviF7q2P3a9RSAioWeYwiC777F9J2eWbjVWqBbNfKboH8P1E0R1oBXCYaE9l3unIP0O9lddan/mFmRaStrOzBSD3PWiYPGaWG4rXZ5ZE7ql38+6aaJHQgmeZr4iwXeRtkl9hNR8BfJD+/GgsstiDO9KDi1r757dMlMca02d7s8cNw3jCzINJ273eZOFbIbOtjWJvAdKA3x8dV62QPQoyxkDo9BlnUrbzsAkQ2ujNnZM9uCtW4lLptcy/Im52iaH9oi5UyoyaZp1Pj0/K29+zZBNFSVdMxUYkeEie1gtoRgATDJ4mpW5JvF0a5GlwPl10p5sAEWnknc6mousb9EEZxngIYa7WNeU7S877JQqWG+FivGqZJ2TV7X8230hD964IEICqPQdPqpgKJvU8UXSsFQMCyMtMpu3A1NTv4PsPZS3CEUoK/vQAAAAASUVORK5CYII=') no-repeat scroll center center transparent !important;
	border: none;
	width: 16px;
	margin-right: 2px;
	outline:0 !important;
 }
 
 div.gcView{
 	clear:both;
 }
 div.gcView:after{
 	content:'';
 	display:block;
 	clear:both;
 }
</style>
<script type="text/javascript">
$(function () {
	
	var docCheckId = "14918761353119285";
	
	var gcUrl = "http://www.paperfree.cn/fragment/user_gc.html";

    $("div[align=right]>*").click(function(e){
        var $this = $(this);

        if($(e.target).is(":not(input)")){
            $this.closest("div").find("input").click();
        }
    });
	
	var partContent = "";
	window.submitPart = function (_partContent) {
	 	partContent = _partContent;
	}
	
	$("div[align=right] input").click(_delay(function(e){
		 var $this = $(this);
        var $right = $this.closest("div[align=right]");
        var $gcView = $right.data("gcView");

        if (!$gcView) {
            $gcView = $("<div class='gcView' />").hide().insertAfter($right);
            $right.data("gcView", $gcView);

            var $form = $("<form target='_blank' method='post' />").appendTo($gcView).attr("action", gcUrl+"");
            $("<input type='hidden' name='docCheckId' />").appendTo($form).val(docCheckId+"");
            
            $("<p>改重内容（请对本段修改之后，点击“提交片段检测”，然后在“降重结果”页面中获取实时检测结果）：</p>").css({
                "color": "#666",
                "margin": ".5em 0 3px 0",
                "font": "99% 'Helvetica Neue', Arial, Helvetica, san-serif"
            }).appendTo($form);

            $('<textarea name="content" rows="10" cols="20" id="paper_content" style="width:99%" value=""></textarea>').css({
                "color": "#000",
                "padding": ".3em",
                "float": "none",
                "margin-bottom": ".1em",
                "font": "99% 'Helvetica Neue', Arial, Helvetica, san-serif"
            }).appendTo($form);

            $('<p><input type="submit" name="Submit" value="提交片段检测" /></p>').css({
                "padding": "0 0px 0px 0",
                "text-align": "right"
            }).appendTo($form);

            $gcView.find("textarea").val(partContent);

            $form.submit(function(){
                $(this).parent().slideUp("fast");
            });
        }

        if ($gcView.is(":animated")) {
        } else if ($gcView.is(":visible")) {
            $gcView.slideUp("fast");
        } else {
            $gcView.slideDown("fast").promise().done(function () {
                $(this).find("textarea").focus();
            });
        }

        return false;
	}, 10));
	
	function _delay(callback, time){
		return function(e){
			var _this = this;
			setTimeout(function(){
			   callback.call(_this, e);
			   _this = null;
			}, time);
		}
	}
});
</script>

<!--增加的代码------------------结束-->

</body>
</html>
